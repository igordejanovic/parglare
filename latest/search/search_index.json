{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"A pure Python LR/GLR parser. Feature highlights \u00b6 Integrated scanner There is no lexing as a separate phase. There is no separate lexer grammar. The parser will try to recognize token during parsing at the given location. This brings more parsing power as there are no lexical ambiguities introduced by a separate lexing stage. You want variables in your language to be named like some of the keywords? No problem. Generalized parsing - GLR parglare gives you powerful tools to see where non-determinism in your grammar lies (the notorious shift-reduce and reduce-reduce conflicts ) and gives detailed info on why that happened. In case your language needs non-deterministic parsing \u2014 either it needs additional lookahead to decide or your language is inherently ambiguous \u2014 you can resort to the GLR algorithm by a simple change of the parser class. The grammar stays the same. In the case of non-determinism (inability for a parser to deterministically decide what to do) the parser will fork and investigate each possibility. Eventually, parsers that decided wrong will die leaving only the right one. In case there are multiple interpretation of your input you will get all the trees (a.k.a. \"the parse forest\"). Declarative associativity and priority rules These problems arise a lot when building expression languages. Even an arithmetic expression as small as 3 + 4 * 5 * 2 have multiple interpretation depending on the associativity and priority of operations. In parglare it is easy to specify these rules in the grammar (see the quick intro below or the calc example ). Tracing/debugging, visualization and error reporting There is an extensive support for grammar checking, debugging, automata visualization, and parse tracing. Check out pglr command . Parsing an arbitrary list of objects parglare is not used only to parse a textual content. It can parse (create a tree) of an arbitrary list of objects (numbers, bytes, whatever) based on a common parglare grammar. For this you have to define token recognizers for your input stream. The built-in recognizers are string and regex recognizers for parsing textual inputs. See recognizers parameter to grammar construction in the test_recognizers.py test . Flexible actions calling strategies During parsing you will want to do something when the grammar rule matches. The whole point of parsing is that you want to transform your input to some output. There are several options: by default parser builds nested lists; you can build a tree using build_tree=True parameter to the parser; call user-supplied actions - you write a Python function that is called when the rule matches. You can do whatever you want at this place and the result returned is used in parent rules/actions. There are some handy build-in actions in the parglare.actions module . User actions may be postponed and called on the parse tree - this is handy if you want to process your tree in multiple ways, or you are using GLR parsing and the actions are introducing side-effects and you would like to avoid those effects created from wrong parsers/trees. Grammar modularization Grammars can be split in multiple files and imported where needed. In addition each grammar file may have an actions and recognizers python file defined. This enable a nice separation of parts of the language with their grammars and accompanying actions and recognizers. Support for whitespaces/comments Support for language comments/whitespaces is done using the special rule LAYOUT . By default whitespaces are skipped. This is controlled by ws parameter to the parser constructor which is by default set to \\t\\n . If set to None no whitespace skipping is provided. If there is a rule LAYOUT in the grammar this rule is used instead. An additional parser with the grammar defined by the LAYOUT rule will be built to handle whitespaces. Error recovery This is something that often lacks in parsing libraries. More often than not you will want your parser to recover from an error, report it, and continue parsing. parglare has a built-in error recovery strategy which is currently a simplistic one -- it will skip current character until it is able to continue -- but gives you possibility to provide your own. You will write a strategy that will either skip input or introduce non-existing but expected tokens. Test coverage Test coverage is high and I'll try to keep it that way. Tip This documentation is versioned. In the upper left corner choose your version. latest is the version that follows master branch from the git repo. TODO/Planed \u00b6 For a detailed list see here Install \u00b6 Stable version: $ pip install parglare Development version: $ git clone git@github.com:igordejanovic/parglare.git $ pip install -e parglare Quick intro \u00b6 This is just a small example to get the general idea. This example shows how to parse and evaluate expressions with 5 operations with different priority and associativity. Evaluation is done using semantic/reduction actions. The whole expression evaluator is done in under 30 lines of code! from parglare import Parser, Grammar grammar = r\"\"\" E: E '+' E {left, 1} | E '-' E {left, 1} | E '*' E {left, 2} | E '/' E {left, 2} | E '^' E {right, 3} | '(' E ')' | number; terminals number: /\\d+(\\.\\d+)?/; \"\"\" actions = { \"E\": [lambda _, n: n[0] + n[2], lambda _, n: n[0] - n[2], lambda _, n: n[0] * n[2], lambda _, n: n[0] / n[2], lambda _, n: n[0] ** n[2], lambda _, n: n[1], lambda _, n: n[0]], \"number\": lambda _, value: float(value), } g = Grammar.from_string(grammar) parser = Parser(g, debug=True, actions=actions) result = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\") print(\"Result = \", result) # Output # -- Debugging/tracing output with detailed info about grammar, productions, # -- terminals and nonterminals, DFA states, parsing progress, # -- and at the end of the output: # Result = 700.8 Note LR tables calculation parglare provides both SLR and LALR tables calculation (LALR is the default). LALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although not proven, this should enable handling of all LR(1) grammars with reduced set of states and without conflicts. For grammars that are not LR(1) a GLR parsing is provided. If a grammar is loaded from file, its table will be persisted between runs in .pgt file. To generate .pgt file explicitly use pglr compile command. Citing parglare \u00b6 If you use parglare in your research please cite this paper: Igor Dejanovi\u0107, Parglare: A LR/GLR parser for Python, Science of Computer Programming, issn:0167-6423, p.102734, DOI:10.1016/j.scico.2021.102734, 2021. @article{dejanovic2021b, author = {Igor Dejanovi\u0107}, title = {Parglare: A LR/GLR parser for Python}, doi = {10.1016/j.scico.2021.102734}, issn = {0167-6423}, journal = {Science of Computer Programming}, keywords = {parsing, LR, GLR, Python, visualization}, pages = {102734}, url = {https://www.sciencedirect.com/science/article/pii/S0167642321001271}, year = {2021} } What does parglare mean? \u00b6 It is an amalgam of the words parser and glare where the second word is chosen to contain letters GLR and to be easy for pronunciation. I also like one of the translations for the word - to be very bright and intense (by The Free Dictionary ) Oh, and the name is non-generic and unique which make it easy to find on the net. ;) The project logo is my take (not very successful) on drawing the Hydra, a creature with multiple heads from the Greek mythology. According to the legend, the Hydra had a regeneration feature: for every head chopped off, the Hydra would regrow a couple of new heads. That reminds me a lot of the GLR parsing ;) License \u00b6 MIT Python versions \u00b6 Tested with 3.6-3.10","title":"Home"},{"location":"#feature-highlights","text":"Integrated scanner There is no lexing as a separate phase. There is no separate lexer grammar. The parser will try to recognize token during parsing at the given location. This brings more parsing power as there are no lexical ambiguities introduced by a separate lexing stage. You want variables in your language to be named like some of the keywords? No problem. Generalized parsing - GLR parglare gives you powerful tools to see where non-determinism in your grammar lies (the notorious shift-reduce and reduce-reduce conflicts ) and gives detailed info on why that happened. In case your language needs non-deterministic parsing \u2014 either it needs additional lookahead to decide or your language is inherently ambiguous \u2014 you can resort to the GLR algorithm by a simple change of the parser class. The grammar stays the same. In the case of non-determinism (inability for a parser to deterministically decide what to do) the parser will fork and investigate each possibility. Eventually, parsers that decided wrong will die leaving only the right one. In case there are multiple interpretation of your input you will get all the trees (a.k.a. \"the parse forest\"). Declarative associativity and priority rules These problems arise a lot when building expression languages. Even an arithmetic expression as small as 3 + 4 * 5 * 2 have multiple interpretation depending on the associativity and priority of operations. In parglare it is easy to specify these rules in the grammar (see the quick intro below or the calc example ). Tracing/debugging, visualization and error reporting There is an extensive support for grammar checking, debugging, automata visualization, and parse tracing. Check out pglr command . Parsing an arbitrary list of objects parglare is not used only to parse a textual content. It can parse (create a tree) of an arbitrary list of objects (numbers, bytes, whatever) based on a common parglare grammar. For this you have to define token recognizers for your input stream. The built-in recognizers are string and regex recognizers for parsing textual inputs. See recognizers parameter to grammar construction in the test_recognizers.py test . Flexible actions calling strategies During parsing you will want to do something when the grammar rule matches. The whole point of parsing is that you want to transform your input to some output. There are several options: by default parser builds nested lists; you can build a tree using build_tree=True parameter to the parser; call user-supplied actions - you write a Python function that is called when the rule matches. You can do whatever you want at this place and the result returned is used in parent rules/actions. There are some handy build-in actions in the parglare.actions module . User actions may be postponed and called on the parse tree - this is handy if you want to process your tree in multiple ways, or you are using GLR parsing and the actions are introducing side-effects and you would like to avoid those effects created from wrong parsers/trees. Grammar modularization Grammars can be split in multiple files and imported where needed. In addition each grammar file may have an actions and recognizers python file defined. This enable a nice separation of parts of the language with their grammars and accompanying actions and recognizers. Support for whitespaces/comments Support for language comments/whitespaces is done using the special rule LAYOUT . By default whitespaces are skipped. This is controlled by ws parameter to the parser constructor which is by default set to \\t\\n . If set to None no whitespace skipping is provided. If there is a rule LAYOUT in the grammar this rule is used instead. An additional parser with the grammar defined by the LAYOUT rule will be built to handle whitespaces. Error recovery This is something that often lacks in parsing libraries. More often than not you will want your parser to recover from an error, report it, and continue parsing. parglare has a built-in error recovery strategy which is currently a simplistic one -- it will skip current character until it is able to continue -- but gives you possibility to provide your own. You will write a strategy that will either skip input or introduce non-existing but expected tokens. Test coverage Test coverage is high and I'll try to keep it that way. Tip This documentation is versioned. In the upper left corner choose your version. latest is the version that follows master branch from the git repo.","title":"Feature highlights"},{"location":"#todoplaned","text":"For a detailed list see here","title":"TODO/Planed"},{"location":"#install","text":"Stable version: $ pip install parglare Development version: $ git clone git@github.com:igordejanovic/parglare.git $ pip install -e parglare","title":"Install"},{"location":"#quick-intro","text":"This is just a small example to get the general idea. This example shows how to parse and evaluate expressions with 5 operations with different priority and associativity. Evaluation is done using semantic/reduction actions. The whole expression evaluator is done in under 30 lines of code! from parglare import Parser, Grammar grammar = r\"\"\" E: E '+' E {left, 1} | E '-' E {left, 1} | E '*' E {left, 2} | E '/' E {left, 2} | E '^' E {right, 3} | '(' E ')' | number; terminals number: /\\d+(\\.\\d+)?/; \"\"\" actions = { \"E\": [lambda _, n: n[0] + n[2], lambda _, n: n[0] - n[2], lambda _, n: n[0] * n[2], lambda _, n: n[0] / n[2], lambda _, n: n[0] ** n[2], lambda _, n: n[1], lambda _, n: n[0]], \"number\": lambda _, value: float(value), } g = Grammar.from_string(grammar) parser = Parser(g, debug=True, actions=actions) result = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\") print(\"Result = \", result) # Output # -- Debugging/tracing output with detailed info about grammar, productions, # -- terminals and nonterminals, DFA states, parsing progress, # -- and at the end of the output: # Result = 700.8 Note LR tables calculation parglare provides both SLR and LALR tables calculation (LALR is the default). LALR is modified to avoid REDUCE/REDUCE conflicts on state merging. Although not proven, this should enable handling of all LR(1) grammars with reduced set of states and without conflicts. For grammars that are not LR(1) a GLR parsing is provided. If a grammar is loaded from file, its table will be persisted between runs in .pgt file. To generate .pgt file explicitly use pglr compile command.","title":"Quick intro"},{"location":"#citing-parglare","text":"If you use parglare in your research please cite this paper: Igor Dejanovi\u0107, Parglare: A LR/GLR parser for Python, Science of Computer Programming, issn:0167-6423, p.102734, DOI:10.1016/j.scico.2021.102734, 2021. @article{dejanovic2021b, author = {Igor Dejanovi\u0107}, title = {Parglare: A LR/GLR parser for Python}, doi = {10.1016/j.scico.2021.102734}, issn = {0167-6423}, journal = {Science of Computer Programming}, keywords = {parsing, LR, GLR, Python, visualization}, pages = {102734}, url = {https://www.sciencedirect.com/science/article/pii/S0167642321001271}, year = {2021} }","title":"Citing parglare"},{"location":"#what-does-parglare-mean","text":"It is an amalgam of the words parser and glare where the second word is chosen to contain letters GLR and to be easy for pronunciation. I also like one of the translations for the word - to be very bright and intense (by The Free Dictionary ) Oh, and the name is non-generic and unique which make it easy to find on the net. ;) The project logo is my take (not very successful) on drawing the Hydra, a creature with multiple heads from the Greek mythology. According to the legend, the Hydra had a regeneration feature: for every head chopped off, the Hydra would regrow a couple of new heads. That reminds me a lot of the GLR parsing ;)","title":"What does parglare mean?"},{"location":"#license","text":"MIT","title":"License"},{"location":"#python-versions","text":"Tested with 3.6-3.10","title":"Python versions"},{"location":"actions/","text":"Actions \u00b6 Actions (a.k.a. semantic actions or reductions actions ) are Python callables (functions or lambdas mostly) that get called to reduce the recognized pattern to some higher concept. E.g. in the calc example actions are called to calculate sub-expressions. Note LR parser can call the actions during parsing. GLR parser always build the parse forest and the actions can be called afterwards on a chosen tree with parser.call_actions . There are two consideration to think of: Which actions are called? When actions are called? Custom actions and built-in actions \u00b6 If you don't provide actions of your own the parser will return nested list corresponding to your grammar. Each non-terminal result in a list of evaluated sub-expression while each terminal result in the matched string. If the parser parameter build_tree is set to True the parser will build a parse tree . Custom actions are provided to the parser during parser instantiation as actions parameter which must be a Python dict where the keys are the names of the rules from the grammar and values are the action callables or a list of callables if the rule has more than one production/choice. You can provide additional actions that are not named after the grammar rule names, these actions may be referenced from the grammar using @ syntax for action specification . Lets take a closer look at the quick intro example: grammar = r\"\"\" E: E '+' E {left, 1} | E '-' E {left, 1} | E '*' E {left, 2} | E '/' E {left, 2} | E '^' E {right, 3} | '(' E ')' | number; terminals number: /\\d+(\\.\\d+)?/; \"\"\" actions = { \"E\": [lambda _, nodes: nodes[0] + nodes[2], lambda _, nodes: nodes[0] - nodes[2], lambda _, nodes: nodes[0] * nodes[2], lambda _, nodes: nodes[0] / nodes[2], lambda _, nodes: nodes[0] ** nodes[2], lambda _, nodes: nodes[1], lambda _, nodes: nodes[0]], \"number\": lambda _, value: float(value), } g = Grammar.from_string(grammar) parser = Parser(g, actions=actions) result = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\") Here you can see that for rule E we provide a list of lambdas, one lambda for each operation. The first element of the list corresponds to the first production of the E rule ( E '+' E {left, 1} ), the second to the second and so on. For number rule there is only a single lambda which converts the matched string to the Python float type, because number is a terminal definition and thus the second parameter in action call will not be a list but a matched value itself. At the end we instantiate the parser and pass in our actions using the parameter. Each action callable receive two parameters. The first is the context object which gives parsing context information (like the start and end position where the match occurred, the parser instance etc.). The second parameters nodes is a list of actual results of sub-expressions given in the order defined in the grammar. For example: lambda _, nodes: nodes[0] * nodes[2], In this line we don't care about the context thus giving it the _ name. nodes[0] will cary the value of the left sub-expression while nodes[2] will carry the result of the right sub-expression. nodes[1] must be * and we don't need to check that as the parser already did that for us. The result of the parsing will be the evaluated expression as the actions will get called along the way and the result of each actions will be used as an element of the nodes parameter in calling actions higher in the hierarchy. If we don't provide actions , by default parglare will return a matched string for each terminal and a list of sub-expressions for each non-terminal effectively producing nested lists. If we set build_tree parameter of the parser to True the parser will produce a parse tree whose elements are instances of NodeNonTerm and NodeTerm classes representing a non-terminals and terminals respectively. action decorator \u00b6 You can use a special decorator/collector factory parglare.get_collector to create decorator that can be used to collect all actions. from parglare import get_collector action = get_collector() @action def number(_, value): return float(value) @action('E') def sum_act(_, nodes): return nodes[0] + nodes[2] @action('E') def pass_act_E(_, nodes): return nodes[0] @action def T(_, nodes): if len(nodes) == 3: return nodes[0] * nodes[2] else: return nodes[0] @action('F') def parenthesses_act(_, nodes): return nodes[1] @action('F') def pass_act_F(_, nodes): return nodes[0] p = Parser(grammar, actions=action.all) In the previous example action decorator is created using get_collector factory. This decorator is parametrized where optional parameter is the name of the action. If the name is not given the name of the decorated function will be used. As you can see in the previous example, same name can be used multiple times (e.g. E for sum_act and pass_act_E ). If same name is used multiple times all action functions will be collected as a list in the order of definition. Dictionary holding all actions for the created action decorator is action.all . Time of actions call \u00b6 Note This applies for LR parsing only. GLR parser always build a forest and actions are called afterwards with parser.call_actions . In parglare actions can be called during parsing (i.e. on the fly) which you could use if you want to transform input immediately without building the parse tree. But there are times when you would like to build a tree first and call actions afterwards. To get the tree and call actions afterwards you supply actions parameter to the parser as usual and set build_tree to True . When the parser finishes successfully it will return the parse tree which you pass to the call_actions method of the parser object to execute actions. For example: parser = Parser(g, actions=actions) tree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\") result = parser.call_actions(tree) Built-in actions \u00b6 parglare provides some common actions in the module parglare.actions . You can reference these actions directly from the grammar . Built-in actions are used implicitly by parglare as default actions in particular case (e.g. for syntactic sugar ) but you might want to reference some of these actions directly. Following are parglare built-in actions from the parglare.actions module: pass_none - returns None ; pass_nochange - returns second parameter of action callable ( value or nodes ) unchanged; pass_empty - returns an empty list [] ; pass_single - returns nodes[0] . Used implicitly by rules where all productions have only a single rule reference on the RHS; pass_inner - returns nodes[1:-1] or nodes[1] if len(nodes)==3 . Handy to strip surrounding parentheses; collect - Used for rules of the form Elements: Elements Element | Element; . Implicitly used for + operator. Returns list; collect_sep - Used for rules of the form Elements: Elements separator Element | Element; . Implicitly used for + with separator. Returns list; collect_optional - Can be used for rules of the form Elements: Elements Element | Element | EMPTY; . Returns list; collect_sep_optional - Can be used for rules of the form Elements: Elements separator Element | Element | EMPTY; . Returns list; collect_right - Can be used for rules of the form Elements: Element Elements | Element; . Returns list; collect_right_sep - Can be used for rules of the form Elements: Element separator Elements | Element; . Returns list; collect_right_optional - Can be used for rules of the form Elements: Element Elements | Element | EMPTY; . Returns list; collect_right_sep_optional - Can be used for rules of the form Elements: Element separator Elements | Element | EMPTY; . Returns list; optional - Used for rules of the form OptionalElement: Element | EMPTY; . Implicitly used for ? operator. Returns either a sub-expression value or None if empty match. obj - Used implicitly by rules using named matches . Creates Python object with attributes derived from named matches. Objects created this way have additional attributes _pg_start_position / _pg_end_position with start/end position in the input stream where the object is found. Actions for rules using named matches \u00b6 If named matches are used in the grammar rule, action will be called with additional keyword parameters named by the name of LHS of rule assignments. If no action is specified for the rule a built-in action obj is called and will produce instance of dynamically created Python class corresponding to the grammar rule. See more in the section on named matches . Dynamically created Python objects will have attributes created from assignments in the grammar rules. Also, a special _pg_children attribute is provided with child nodes in the order as they are matched in the input. This may be useful for tree iteration order. Please see this test for an example. In addition, _pg_children_names is a list of attribute names (i.e. a LHS of the assignments in the grammar.). Each created object has a to_str() method which produce a nice textual tree representation. For performance reasons, AST nodes created with the default obj action uses slots so no dynamic attribute creation is possible. However, there is uninitialized _pg_extras attribute which can be used to add additional user-defined information on AST nodes. If for some reason you want to override the default behavior which creates Python object you can create an action like this: S: first=a second=digit+[comma]; terminals a: \"a\"; digit: /\\d+/; now create an action function that accepts additional params: def s_action(context, nodes, first, second): ... do some transformation and return the result of S evaluation ... nodes will contain subexpression results by position while ... first and second will contain the values of corresponding ... sub-expressions register action on Parser instance as usual: parser = Parser(grammar, actions={\"S\": s_action})","title":"Actions"},{"location":"actions/#actions","text":"Actions (a.k.a. semantic actions or reductions actions ) are Python callables (functions or lambdas mostly) that get called to reduce the recognized pattern to some higher concept. E.g. in the calc example actions are called to calculate sub-expressions. Note LR parser can call the actions during parsing. GLR parser always build the parse forest and the actions can be called afterwards on a chosen tree with parser.call_actions . There are two consideration to think of: Which actions are called? When actions are called?","title":"Actions"},{"location":"actions/#custom-actions-and-built-in-actions","text":"If you don't provide actions of your own the parser will return nested list corresponding to your grammar. Each non-terminal result in a list of evaluated sub-expression while each terminal result in the matched string. If the parser parameter build_tree is set to True the parser will build a parse tree . Custom actions are provided to the parser during parser instantiation as actions parameter which must be a Python dict where the keys are the names of the rules from the grammar and values are the action callables or a list of callables if the rule has more than one production/choice. You can provide additional actions that are not named after the grammar rule names, these actions may be referenced from the grammar using @ syntax for action specification . Lets take a closer look at the quick intro example: grammar = r\"\"\" E: E '+' E {left, 1} | E '-' E {left, 1} | E '*' E {left, 2} | E '/' E {left, 2} | E '^' E {right, 3} | '(' E ')' | number; terminals number: /\\d+(\\.\\d+)?/; \"\"\" actions = { \"E\": [lambda _, nodes: nodes[0] + nodes[2], lambda _, nodes: nodes[0] - nodes[2], lambda _, nodes: nodes[0] * nodes[2], lambda _, nodes: nodes[0] / nodes[2], lambda _, nodes: nodes[0] ** nodes[2], lambda _, nodes: nodes[1], lambda _, nodes: nodes[0]], \"number\": lambda _, value: float(value), } g = Grammar.from_string(grammar) parser = Parser(g, actions=actions) result = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\") Here you can see that for rule E we provide a list of lambdas, one lambda for each operation. The first element of the list corresponds to the first production of the E rule ( E '+' E {left, 1} ), the second to the second and so on. For number rule there is only a single lambda which converts the matched string to the Python float type, because number is a terminal definition and thus the second parameter in action call will not be a list but a matched value itself. At the end we instantiate the parser and pass in our actions using the parameter. Each action callable receive two parameters. The first is the context object which gives parsing context information (like the start and end position where the match occurred, the parser instance etc.). The second parameters nodes is a list of actual results of sub-expressions given in the order defined in the grammar. For example: lambda _, nodes: nodes[0] * nodes[2], In this line we don't care about the context thus giving it the _ name. nodes[0] will cary the value of the left sub-expression while nodes[2] will carry the result of the right sub-expression. nodes[1] must be * and we don't need to check that as the parser already did that for us. The result of the parsing will be the evaluated expression as the actions will get called along the way and the result of each actions will be used as an element of the nodes parameter in calling actions higher in the hierarchy. If we don't provide actions , by default parglare will return a matched string for each terminal and a list of sub-expressions for each non-terminal effectively producing nested lists. If we set build_tree parameter of the parser to True the parser will produce a parse tree whose elements are instances of NodeNonTerm and NodeTerm classes representing a non-terminals and terminals respectively.","title":"Custom actions and built-in actions"},{"location":"actions/#action-decorator","text":"You can use a special decorator/collector factory parglare.get_collector to create decorator that can be used to collect all actions. from parglare import get_collector action = get_collector() @action def number(_, value): return float(value) @action('E') def sum_act(_, nodes): return nodes[0] + nodes[2] @action('E') def pass_act_E(_, nodes): return nodes[0] @action def T(_, nodes): if len(nodes) == 3: return nodes[0] * nodes[2] else: return nodes[0] @action('F') def parenthesses_act(_, nodes): return nodes[1] @action('F') def pass_act_F(_, nodes): return nodes[0] p = Parser(grammar, actions=action.all) In the previous example action decorator is created using get_collector factory. This decorator is parametrized where optional parameter is the name of the action. If the name is not given the name of the decorated function will be used. As you can see in the previous example, same name can be used multiple times (e.g. E for sum_act and pass_act_E ). If same name is used multiple times all action functions will be collected as a list in the order of definition. Dictionary holding all actions for the created action decorator is action.all .","title":"action decorator"},{"location":"actions/#time-of-actions-call","text":"Note This applies for LR parsing only. GLR parser always build a forest and actions are called afterwards with parser.call_actions . In parglare actions can be called during parsing (i.e. on the fly) which you could use if you want to transform input immediately without building the parse tree. But there are times when you would like to build a tree first and call actions afterwards. To get the tree and call actions afterwards you supply actions parameter to the parser as usual and set build_tree to True . When the parser finishes successfully it will return the parse tree which you pass to the call_actions method of the parser object to execute actions. For example: parser = Parser(g, actions=actions) tree = parser.parse(\"34 + 4.6 / 2 * 4^2^2 + 78\") result = parser.call_actions(tree)","title":"Time of actions call"},{"location":"actions/#built-in-actions","text":"parglare provides some common actions in the module parglare.actions . You can reference these actions directly from the grammar . Built-in actions are used implicitly by parglare as default actions in particular case (e.g. for syntactic sugar ) but you might want to reference some of these actions directly. Following are parglare built-in actions from the parglare.actions module: pass_none - returns None ; pass_nochange - returns second parameter of action callable ( value or nodes ) unchanged; pass_empty - returns an empty list [] ; pass_single - returns nodes[0] . Used implicitly by rules where all productions have only a single rule reference on the RHS; pass_inner - returns nodes[1:-1] or nodes[1] if len(nodes)==3 . Handy to strip surrounding parentheses; collect - Used for rules of the form Elements: Elements Element | Element; . Implicitly used for + operator. Returns list; collect_sep - Used for rules of the form Elements: Elements separator Element | Element; . Implicitly used for + with separator. Returns list; collect_optional - Can be used for rules of the form Elements: Elements Element | Element | EMPTY; . Returns list; collect_sep_optional - Can be used for rules of the form Elements: Elements separator Element | Element | EMPTY; . Returns list; collect_right - Can be used for rules of the form Elements: Element Elements | Element; . Returns list; collect_right_sep - Can be used for rules of the form Elements: Element separator Elements | Element; . Returns list; collect_right_optional - Can be used for rules of the form Elements: Element Elements | Element | EMPTY; . Returns list; collect_right_sep_optional - Can be used for rules of the form Elements: Element separator Elements | Element | EMPTY; . Returns list; optional - Used for rules of the form OptionalElement: Element | EMPTY; . Implicitly used for ? operator. Returns either a sub-expression value or None if empty match. obj - Used implicitly by rules using named matches . Creates Python object with attributes derived from named matches. Objects created this way have additional attributes _pg_start_position / _pg_end_position with start/end position in the input stream where the object is found.","title":"Built-in actions"},{"location":"actions/#actions-for-rules-using-named-matches","text":"If named matches are used in the grammar rule, action will be called with additional keyword parameters named by the name of LHS of rule assignments. If no action is specified for the rule a built-in action obj is called and will produce instance of dynamically created Python class corresponding to the grammar rule. See more in the section on named matches . Dynamically created Python objects will have attributes created from assignments in the grammar rules. Also, a special _pg_children attribute is provided with child nodes in the order as they are matched in the input. This may be useful for tree iteration order. Please see this test for an example. In addition, _pg_children_names is a list of attribute names (i.e. a LHS of the assignments in the grammar.). Each created object has a to_str() method which produce a nice textual tree representation. For performance reasons, AST nodes created with the default obj action uses slots so no dynamic attribute creation is possible. However, there is uninitialized _pg_extras attribute which can be used to add additional user-defined information on AST nodes. If for some reason you want to override the default behavior which creates Python object you can create an action like this: S: first=a second=digit+[comma]; terminals a: \"a\"; digit: /\\d+/; now create an action function that accepts additional params: def s_action(context, nodes, first, second): ... do some transformation and return the result of S evaluation ... nodes will contain subexpression results by position while ... first and second will contain the values of corresponding ... sub-expressions register action on Parser instance as usual: parser = Parser(grammar, actions={\"S\": s_action})","title":"Actions for rules using named matches"},{"location":"common/","text":"Common classes and functions \u00b6 The Context object \u00b6 An object of this kind object is passed to various callback functions (actions, recognizers, error recovery etc.). It is not always an instance of the same class, but all context objects have the following properties: start_position/end_position - the beginning and the end in the input stream where the match occurred. start_position is the location of the first element/character in the input while the end_position is one past the last element/character of the match. Thus end_position - start_position will give the length of the match including the layout. You can use parglare.pos_to_line_col(input, position) function to get line and column of the position. This function returns a tuple (line, column) . file_name - the name/path of the file being parsed. None if Python string is parsed. input_str - the input string (or list of objects) that is being parsed. layout_content - is the layout (whitespaces, comments etc.) that are collected from the previous non-layout match. layout_content_ahead - layout content before token_ahead . token - the token shifted during SHIFT operation. Instance of parglare.parser.Token . token_ahead - the token recognized as a lookahead. production - an instance of parglare.grammar.Production class available only on reduction actions (not on shifts). Represents the grammar production. state - An instance of parglare.tables.LRState . The LR state of the parser automata. This object contains information of the possible actions in this state. node - this is available only if the actions are called over the parse tree using call_actions . It represents the instance of NodeNonTerm or NodeTerm classes from the parse tree where the actions is executed. parser - is the reference to the parser instance. You should use this only to investigate parser configuration not to alter its state. head - is a reference to the Graph-structured stack node ( GSSNode ). Only used for GLR parsing. extra - this attribute can store arbitrary user information for state tracking. If not given as a parameter to parse call a dict is used. Location class \u00b6 Used at various places in parglare to define location and span in the files (e.g. for error reporting). Attributes \u00b6 input_str - the input string being parsed. file_name (property) - the name of the file being parsed ( None if string is parsed), start_position/end_position - an absolute position in the input where the span starts/ends, line / column (properties) - line and column where the span starts. line_end / column_end (properties) - line and column where the span ends. If there is an error in the grammar itself parglare will raise parglare.GrammarError exception.","title":"Common API"},{"location":"common/#common-classes-and-functions","text":"","title":"Common classes and functions"},{"location":"common/#the-context-object","text":"An object of this kind object is passed to various callback functions (actions, recognizers, error recovery etc.). It is not always an instance of the same class, but all context objects have the following properties: start_position/end_position - the beginning and the end in the input stream where the match occurred. start_position is the location of the first element/character in the input while the end_position is one past the last element/character of the match. Thus end_position - start_position will give the length of the match including the layout. You can use parglare.pos_to_line_col(input, position) function to get line and column of the position. This function returns a tuple (line, column) . file_name - the name/path of the file being parsed. None if Python string is parsed. input_str - the input string (or list of objects) that is being parsed. layout_content - is the layout (whitespaces, comments etc.) that are collected from the previous non-layout match. layout_content_ahead - layout content before token_ahead . token - the token shifted during SHIFT operation. Instance of parglare.parser.Token . token_ahead - the token recognized as a lookahead. production - an instance of parglare.grammar.Production class available only on reduction actions (not on shifts). Represents the grammar production. state - An instance of parglare.tables.LRState . The LR state of the parser automata. This object contains information of the possible actions in this state. node - this is available only if the actions are called over the parse tree using call_actions . It represents the instance of NodeNonTerm or NodeTerm classes from the parse tree where the actions is executed. parser - is the reference to the parser instance. You should use this only to investigate parser configuration not to alter its state. head - is a reference to the Graph-structured stack node ( GSSNode ). Only used for GLR parsing. extra - this attribute can store arbitrary user information for state tracking. If not given as a parameter to parse call a dict is used.","title":"The Context object"},{"location":"common/#location-class","text":"Used at various places in parglare to define location and span in the files (e.g. for error reporting).","title":"Location class"},{"location":"common/#attributes","text":"input_str - the input string being parsed. file_name (property) - the name of the file being parsed ( None if string is parsed), start_position/end_position - an absolute position in the input where the span starts/ends, line / column (properties) - line and column where the span starts. line_end / column_end (properties) - line and column where the span ends. If there is an error in the grammar itself parglare will raise parglare.GrammarError exception.","title":"Attributes"},{"location":"debugging/","text":"Tracing and debugging \u00b6 When the parser doesn't work as expected there are several options. First, you can use pglr command to visualize LR PDA automata and GLR trace. The same command can also be used to print detailed information on the grammar, LR states and conflicts. Printing detailed debug information on grammar can be also achieved by putting the grammar in the debug mode: grammar = Grammar.from_file(file_name, debug=True) For example, calc grammar from the quick intro would give the following output: *** GRAMMAR *** Terminals: EMPTY - * ^ + STOP ( \\d+(\\.\\d+)? number / ) NonTerminals: E S' Productions: 0: S' = E STOP 1: E = E + E 2: E = E - E 3: E = E * E 4: E = E / E 5: E = E ^ E 6: E = ( E ) 7: E = number During grammar object construction, grammar file is parsed using the parglare itself and the grammar object for the new language is constructed. If you want to see the debug output of this process set the parse_debug parameter to True : grammar = Grammar.from_file(file_name, parse_debug=True) If you are using custom recognizers or would like to see the result of each action in debug output then you should put the parser in the debug mode from the code. To put the parser in the debug mode do: parser = Parser(grammar, debug=True) To debug layout grammar do: parser = Parser(grammar, debug_layout=True) GLRParser can produce visual trace . To enable visual tracing set debug and debug_trace to True : parser = GLRParser(grammar, debug=True, debug_trace=True) Debug output and visual trace can be generated using pglr command . For example, parsing expression 1 + 2 * 3 with GLRParser in debug mode will produce the following output (this output is generated by pglr -i trace calc.pg \"1 + 2 * 3\" ): Grammar OK. *** STATES *** State 0 0: S' = . E STOP {} 1: E = . E + E {STOP, ^, ), /, *, -, +} 2: E = . E - E {STOP, ^, ), /, *, -, +} 3: E = . E * E {STOP, ^, ), /, *, -, +} 4: E = . E / E {STOP, ^, ), /, *, -, +} 5: E = . E ^ E {STOP, ^, ), /, *, -, +} 6: E = . ( E ) {STOP, ^, ), /, *, -, +} 7: E = . number {STOP, ^, ), /, *, -, +} GOTO: E->1 ACTIONS: (->SHIFT:2, number->SHIFT:3 .... .... *** PARSING STARTED Skipping whitespaces: '' New position: (1, 0) **REDUCING HEADS Active heads 1: [state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1] Number of trees = 1 Reducing head: state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1 Skipping whitespaces: '' New position: (1, 0) Position: (1, 0) Context: *1 + 2 * 3 Symbols expected: ['(', 'number'] Token(s) ahead: [<number(1)>] New head for shifting: state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1. No more reductions for this head and lookahead token <number(1)>. **SHIFTING HEADS Active heads 1: [state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1] Number of trees = 1 Shifting head: state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1 Position: (1, 0) Context: *1 + 2 * 3 Token(s) ahead: <number(1)> Shift:3 \"1\" at position (1, 0) Action result = type:<class 'parglare.parser.NodeTerm'> value:<Term(start=0, end=1, sym=number, val=\"1\")> New shifted head state=3:number, pos=0, endpos=1, empty=[False,True], parents=0, trees=0. Creating link from head state=3:number, pos=0, endpos=1, empty=[False,False], parents=1, trees=1 to head state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1 .... .... Reducing head: state=1:E, pos=0, endpos=9, token ahead=<STOP()>, empty=[False,False], parents=1, trees=1 Position: (1, 9) Context: 1 + 2 * 3* Symbols expected: ['STOP'] Token(s) ahead: <STOP()> *** SUCCESS!!!! *** 1 sucessful parse(s). Generated file parglare_trace.dot. In addition, a visualization of GLR trace is produced as a Graphviz dot file.","title":"Debugging"},{"location":"debugging/#tracing-and-debugging","text":"When the parser doesn't work as expected there are several options. First, you can use pglr command to visualize LR PDA automata and GLR trace. The same command can also be used to print detailed information on the grammar, LR states and conflicts. Printing detailed debug information on grammar can be also achieved by putting the grammar in the debug mode: grammar = Grammar.from_file(file_name, debug=True) For example, calc grammar from the quick intro would give the following output: *** GRAMMAR *** Terminals: EMPTY - * ^ + STOP ( \\d+(\\.\\d+)? number / ) NonTerminals: E S' Productions: 0: S' = E STOP 1: E = E + E 2: E = E - E 3: E = E * E 4: E = E / E 5: E = E ^ E 6: E = ( E ) 7: E = number During grammar object construction, grammar file is parsed using the parglare itself and the grammar object for the new language is constructed. If you want to see the debug output of this process set the parse_debug parameter to True : grammar = Grammar.from_file(file_name, parse_debug=True) If you are using custom recognizers or would like to see the result of each action in debug output then you should put the parser in the debug mode from the code. To put the parser in the debug mode do: parser = Parser(grammar, debug=True) To debug layout grammar do: parser = Parser(grammar, debug_layout=True) GLRParser can produce visual trace . To enable visual tracing set debug and debug_trace to True : parser = GLRParser(grammar, debug=True, debug_trace=True) Debug output and visual trace can be generated using pglr command . For example, parsing expression 1 + 2 * 3 with GLRParser in debug mode will produce the following output (this output is generated by pglr -i trace calc.pg \"1 + 2 * 3\" ): Grammar OK. *** STATES *** State 0 0: S' = . E STOP {} 1: E = . E + E {STOP, ^, ), /, *, -, +} 2: E = . E - E {STOP, ^, ), /, *, -, +} 3: E = . E * E {STOP, ^, ), /, *, -, +} 4: E = . E / E {STOP, ^, ), /, *, -, +} 5: E = . E ^ E {STOP, ^, ), /, *, -, +} 6: E = . ( E ) {STOP, ^, ), /, *, -, +} 7: E = . number {STOP, ^, ), /, *, -, +} GOTO: E->1 ACTIONS: (->SHIFT:2, number->SHIFT:3 .... .... *** PARSING STARTED Skipping whitespaces: '' New position: (1, 0) **REDUCING HEADS Active heads 1: [state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1] Number of trees = 1 Reducing head: state=0:S', pos=0, endpos=0, empty=[False,True], parents=0, trees=1 Skipping whitespaces: '' New position: (1, 0) Position: (1, 0) Context: *1 + 2 * 3 Symbols expected: ['(', 'number'] Token(s) ahead: [<number(1)>] New head for shifting: state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1. No more reductions for this head and lookahead token <number(1)>. **SHIFTING HEADS Active heads 1: [state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1] Number of trees = 1 Shifting head: state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1 Position: (1, 0) Context: *1 + 2 * 3 Token(s) ahead: <number(1)> Shift:3 \"1\" at position (1, 0) Action result = type:<class 'parglare.parser.NodeTerm'> value:<Term(start=0, end=1, sym=number, val=\"1\")> New shifted head state=3:number, pos=0, endpos=1, empty=[False,True], parents=0, trees=0. Creating link from head state=3:number, pos=0, endpos=1, empty=[False,False], parents=1, trees=1 to head state=0:S', pos=0, endpos=0, token ahead=<number(1)>, empty=[False,True], parents=0, trees=1 .... .... Reducing head: state=1:E, pos=0, endpos=9, token ahead=<STOP()>, empty=[False,False], parents=1, trees=1 Position: (1, 9) Context: 1 + 2 * 3* Symbols expected: ['STOP'] Token(s) ahead: <STOP()> *** SUCCESS!!!! *** 1 sucessful parse(s). Generated file parglare_trace.dot. In addition, a visualization of GLR trace is produced as a Graphviz dot file.","title":"Tracing and debugging"},{"location":"disambiguation/","text":"Disambiguation \u00b6 At each step LR parser has to decide which operation to execute: SHIFT (to consume the next token) or REDUCE (to reduce what it saw previously to some higher level concept). See section on LR parsing and conflicts . Defining language by CFG alone often leads to ambiguous languages. Sometimes this is what we want, i.e. our inputs indeed have multiple interpretation and we want all of them. This is usually true for natural languages. But when we deal with computer languages we want to avoid ambiguity as there should be only one interpretation for each valid input. We want to define unambiguous language. To constrain our grammar and make it define unambiguous language we use so called disambiguation filters . These filters are in charge of choosing the right interpretation/tree when there is ambiguity in the grammar. Even in the simple expression grammar there is ambiguity. For example, 2 + 3 * 4 expression \u2014 in case we know nothing about priorities of arithmetic operations \u2014 can be interpreted in two different ways: (2 + 3) * 4 and 2 + (3 * 4) . Without priorities we would be obliged to use parentheses everywhere to specify the right interpretation. Ambiguity is a one source of conflicts in the LR grammars. The other is limited lookahead. Whatever is the source of conflicts GLRParser can cope with it. In case of ambiguity the parser will return all interpretations possible. In case of a limited lookahead the parser will investigate all possible paths and resolve to the correct interpretation further down the input stream. If our grammar is ambiguous and our language is not that means that we need to constrain our grammar using disambiguation filters to better describe our language. Ideally, we strive for a grammar that describe all valid sentences in our language with a single interpretation for each of them and nothing more. Static disambiguation filters \u00b6 Static disambiguation filters are given in the grammar at the end of the production using {} syntax. There is also a dynamic disambiguation filter that is most powerful and is specified as a Python function. priority \u00b6 Priority is probably the simplest form of disambiguation. It is also the strongest in parglare as it's first checked. It is given as a numeric value where the default is 10. When the parser can't decide what operation to use it will favor the one associated with the production with a higher priority. For example: E: E \"*\" E {2}; E: E \"+\" E {1}; This gives priority of 2 to the production E \"*\" E and 1 to the production E \"+\" E . When parglare needs to decide, e.g. between shifting + or reducing * it saw, it will choose reduce as the multiplication production has higher priority. Priority can also be given to terminal productions. associativity \u00b6 Associativity is used for disambiguation between productions of the same priority. In the grammar fragment above we still have ambiguity for expression: 2 + 3 + 5 There are two interpretations (2 + 3) + 5 and 2 + (3 + 5) . Of course, with arithmetic + operation the result will be the same but that's not true for each operation. Anyway, parse trees will be different so some choice has to be made. In this situation associativity is used. Both + and * in arithmetic are left associative (i.e. the operation is evaluated from left to right). E: E \"*\" E {2, left}; E: E \"+\" E {1, left}; Now, the expression above is not ambiguous anymore. It is interpreted as (2 + 3) + 5 . The associativity given in the grammar is either left or right . Default is no associativity, i.e. associativity is not used for disambiguation decision. Tip Alternatively, you can use keyword shift instead of right and reduce instead of left . nops and nopse \u00b6 These two are not actual filters but markers used to disable prefer_shifts ( nops ) and prefer_shifts_over_empty ( nopse ) set globally during parser construction on a production level. Productions using these markers are not influenced by global parser setting meaning that table construction will not eliminate possible reductions on these productions. Using these markers have sense only for GLR parsing as the LR deterministic parser can't be constructed anyway in case of conflicts. For example: Statements: Statements1 {nops} | EMPTY; prefer \u00b6 This disambiguation filter is applicable to terminal productions only. It will be used to choose the right recognizer/terminal in case of lexical ambiguity . For example: INT: /[-+]?[0-9]+\\b/ {prefer}; FLOAT: /[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\\b/; If in the grammar we have a possibility that both recognizers are tried, both will succeed for input 23 , but we want INT to be chosen in this case. Dynamic disambiguation filter \u00b6 All previously described filters are of static nature, i.e. they are compiled during LR table calculation (by removing erroneous automata transitions) and they don't depend on the parsed input. There are sometimes situations when parsing decision depends on the input. For example, lets say that we need to parse arithmetic expression but our operation priority increase for operations that are introduced later in the input. 1 + 2 * 3 - 4 + 5 Should be parsed as: ((1 + (2 * (3 - 4))) + 5) While 1 - 2 + 3 * 4 + 5 should be parsed as: (1 - ((2 + (3 * 4)) + 5)) As you can see, operations that appears later in the input are of higher priority. In parglare you can implement this dynamic behavior in two steps: First, mark productions in your grammar by dynamic rule: E: E op_sum E {dynamic} | E op_mul E {dynamic} | /\\d+/; op_sum: '+' {dynamic}; op_mul: '*' {dynamic}; This tells parglare that those production are candidates for dynamic ambiguity resolution. Second step is to register a predicate function, during parser construction, that will be used for resolution. This function operates as a filter for actions. It receives the parsing context for the action, the LR automata states between whom the transition is about to occur, and the sub-results in the case of a reduction. The function should return True if the transition is allowed or False otherwise. This function sometimes need to maintain some kind of state. To initialize its state at the beginning it is called with None as parameters. parser = Parser(grammar, dynamic_filter=custom_disambiguation_filter) Where resolution function is of the following form: def custom_disambiguation_filter(context, from_state, to_state, action, production, subresults): \"\"\" Make first operation that appears in the input as lower priority. This demonstrates how priority rule can change dynamically depending on the input. \"\"\" global operations # At the start of parsing this function is called with actions set to None # to give a chance for the strategy to initialize. if action is None: operations = [] return if action is SHIFT: operation = context.token.symbol else: operation = context.token_ahead.symbol actions = from_state.actions[operation] if operation not in operations and operation.name != 'STOP': operations.append(operation) if action is SHIFT: shifts = [a for a in actions if a.action is SHIFT] if not shifts: return False reductions = [a for a in actions if a.action is REDUCE] if not reductions: return True red_op = reductions[0].prod.rhs[1] return operations.index(operation) > operations.index(red_op) elif action is REDUCE: # Current reduction operation red_op = production.rhs[1] # If operation ahead is STOP or is of less or equal priority -> reduce. return ((operation not in operations) or (operations.index(operation) <= operations.index(red_op))) This function is a predicate that will be called for each action for productions marked with dynamic (SHIFT action for dynamic terminal production and REDUCE action for dynamic non-terminal productions). You are provided with enough information to make a custom decision whether to perform or reject the operation. Parameters are: context - the parsing context object . from_state/to_state -- LRState instances for the transition, action - either SHIFT or REDUCE constant from parglare module, production - a production used for the REDUCE operation. Valid only if action is REDUCE. subresults (list) - a sub-results for the reduction. Valid only for REDUCE. The length of this list is equal to len(production.rhs) . For details see test_dynamic_disambiguation_filters.py . Disambiguation of a GLR Forest \u00b6 For GLR forests there is an additional option for a disambiguation. The GLR forest has a disambiguate method that accepts a callable of the following signature: def disam_callable(parent): It accepts the GLR GSS Parent object which has a list of possibilities and the callable should remove all invalid possibilities from the list. The callable is called bottom-up for all Parent object which have more than one possibility. For a full example with comments see this test . Lexical ambiguities \u00b6 There is another source of ambiguities. Parglare uses integrated scanner, thus tokens are determined on the fly. This gives greater lexical disambiguation power but lexical ambiguities might arise nevertheless. Lexical ambiguity is a situation when at some place in the input more than one recognizer match successfully. For example, if in the input we have 3.4 and we expect at this place either an integer or a float. Both of these recognizer can match the input. The integer recognizer would match 3 while the float recognizer would match 3.4 . What should we use? parglare has lexical disambiguation strategy that will use priorities first. If this fails (i.e. all terminals have the same priority) we continue with implicit disambiguation strategy as follows: String recognizers are preferred over regexes (i.e. the most specific match). If we still have multiple matches use longest-match strategy. If more recognizers still match use prefer rule if given. If all else fails raise an exception. In case of GLR, ambiguity will be handled by parser forking, i.e. you will end up with all solutions/trees. Thus, in terminal definition rules we can use priorities to favor some of the recognizers, or we can use prefer to favor recognizer if there are multiple matches of the same length. For example: number: /\\d+/ {15}; or: number: /\\d+/ {prefer}; Note Implicit lexical disambiguation is controlled by lexical_disambiguation parameter passed to Parser / GLRParser constructor. By default, Parser uses implict disambiguation while GLRParser doesn't. In addition, you can also specify that the terminal takes part in dynamic disambiguation: number: /\\d+/ {dynamic}; Custom token recognition and lexical disambiguation \u00b6 In the previous section a built-in parglare lexical disambiguation strategy is explained. There are use-cases when this strategy is not sufficient. For example, if we want to do fuzzy match of tokens and choose the most similar token at the position. parglare solves this problem by enabling you to implement a custom token recognition by registering a callable during parser instantiation that will, during parsing, get all the symbols expected at the current location and return a list of tokens (instances of Token class ) or None / empty list if no symbol is found at the location. This callable is registered during parser instantiation as the parameter custom_token_recognition . parser = Parser( grammar, custom_token_recognition=custom_token_recognition) The callable accepts: context - the parsing context object . get_tokens - a callable used to get the tokens recognized using the default strategy. Called without parameters. Custom disambiguation might decide to return this list if no change is necessary, reduce the list, or extend it with new tokens. See the example below how to return list with a token only if the default recognition doesn't succeed. Returns: a list of Token class instances or None /empty list if no token is found. To instantiate Token pass in the symbol and the value of the token. Value of the token is usually a sub-string of the input string. In the following test Bar and Baz non-terminals are fuzzy matched. The non-terminal with the higher score wins but only if the score is above 0.7. grammar = \"\"\" S: Element+; Element: Bar | Baz | Number; terminals Bar: /Bar. \\d+/; Baz: /Baz. \\d+/; Number: /\\d+/; \"\"\" g = Grammar.from_string(grammar) grammar = [g] def custom_token_recognition(context, get_tokens): \"\"\" Custom token recognition should return a single token that is recognized at the given place in the input string. \"\"\" # Call default token recognition. tokens = get_tokens() if tokens: # If default recognition succeeds use the result. return tokens else: # If no tokens are found do the fuzzy match. matchers = [ lambda x: difflib.SequenceMatcher(None, 'bar.', x.lower()), lambda x: difflib.SequenceMatcher(None, 'baz.', x.lower()) ] symbols = [ grammar[0].get_terminal('Bar'), grammar[0].get_terminal('Baz'), ] # Try to do fuzzy match at the position elem = context.input_str[context.position:context.position+4] elem_num = context.input_str[context.position:] number_matcher = re.compile('[^\\d]*(\\d+)') number_match = number_matcher.match(elem_num) ratios = [] for matcher in matchers: ratios.append(matcher(elem).ratio()) max_ratio_index = ratios.index(max(ratios)) if ratios[max_ratio_index] > 0.7 and number_match.group(1): return [Token(symbols[max_ratio_index], number_match.group())] parser = Parser( g, custom_token_recognition=custom_token_recognition) # Bar and Baz will be recognized by a fuzzy match result = parser.parse('bar. 56 Baz 12') assert result == ['bar. 56', 'Baz 12'] result = parser.parse('Buz. 34 bar 56') assert result == ['Buz. 34', 'bar 56'] result = parser.parse('Ba. 34 baz 56') assert result == ['Ba. 34', 'baz 56'] # But if Bar/Baz are too different from the correct pattern # we get ParseError. In this case `bza` score is below 0.7 # for both Bar and Baz symbols. with pytest.raises(ParseError): parser.parse('Bar. 34 bza 56') Note custom_token_recognition can be used to implement custom lexical disambiguation by calling get_tokens and then reducing returned list to a list with a single result. Tip See the end of the parse trees section for a tip on how to investigate ambiguities in GLR parsing.","title":"Disambiguation"},{"location":"disambiguation/#disambiguation","text":"At each step LR parser has to decide which operation to execute: SHIFT (to consume the next token) or REDUCE (to reduce what it saw previously to some higher level concept). See section on LR parsing and conflicts . Defining language by CFG alone often leads to ambiguous languages. Sometimes this is what we want, i.e. our inputs indeed have multiple interpretation and we want all of them. This is usually true for natural languages. But when we deal with computer languages we want to avoid ambiguity as there should be only one interpretation for each valid input. We want to define unambiguous language. To constrain our grammar and make it define unambiguous language we use so called disambiguation filters . These filters are in charge of choosing the right interpretation/tree when there is ambiguity in the grammar. Even in the simple expression grammar there is ambiguity. For example, 2 + 3 * 4 expression \u2014 in case we know nothing about priorities of arithmetic operations \u2014 can be interpreted in two different ways: (2 + 3) * 4 and 2 + (3 * 4) . Without priorities we would be obliged to use parentheses everywhere to specify the right interpretation. Ambiguity is a one source of conflicts in the LR grammars. The other is limited lookahead. Whatever is the source of conflicts GLRParser can cope with it. In case of ambiguity the parser will return all interpretations possible. In case of a limited lookahead the parser will investigate all possible paths and resolve to the correct interpretation further down the input stream. If our grammar is ambiguous and our language is not that means that we need to constrain our grammar using disambiguation filters to better describe our language. Ideally, we strive for a grammar that describe all valid sentences in our language with a single interpretation for each of them and nothing more.","title":"Disambiguation"},{"location":"disambiguation/#static-disambiguation-filters","text":"Static disambiguation filters are given in the grammar at the end of the production using {} syntax. There is also a dynamic disambiguation filter that is most powerful and is specified as a Python function.","title":"Static disambiguation filters"},{"location":"disambiguation/#priority","text":"Priority is probably the simplest form of disambiguation. It is also the strongest in parglare as it's first checked. It is given as a numeric value where the default is 10. When the parser can't decide what operation to use it will favor the one associated with the production with a higher priority. For example: E: E \"*\" E {2}; E: E \"+\" E {1}; This gives priority of 2 to the production E \"*\" E and 1 to the production E \"+\" E . When parglare needs to decide, e.g. between shifting + or reducing * it saw, it will choose reduce as the multiplication production has higher priority. Priority can also be given to terminal productions.","title":"priority"},{"location":"disambiguation/#associativity","text":"Associativity is used for disambiguation between productions of the same priority. In the grammar fragment above we still have ambiguity for expression: 2 + 3 + 5 There are two interpretations (2 + 3) + 5 and 2 + (3 + 5) . Of course, with arithmetic + operation the result will be the same but that's not true for each operation. Anyway, parse trees will be different so some choice has to be made. In this situation associativity is used. Both + and * in arithmetic are left associative (i.e. the operation is evaluated from left to right). E: E \"*\" E {2, left}; E: E \"+\" E {1, left}; Now, the expression above is not ambiguous anymore. It is interpreted as (2 + 3) + 5 . The associativity given in the grammar is either left or right . Default is no associativity, i.e. associativity is not used for disambiguation decision. Tip Alternatively, you can use keyword shift instead of right and reduce instead of left .","title":"associativity"},{"location":"disambiguation/#nops-and-nopse","text":"These two are not actual filters but markers used to disable prefer_shifts ( nops ) and prefer_shifts_over_empty ( nopse ) set globally during parser construction on a production level. Productions using these markers are not influenced by global parser setting meaning that table construction will not eliminate possible reductions on these productions. Using these markers have sense only for GLR parsing as the LR deterministic parser can't be constructed anyway in case of conflicts. For example: Statements: Statements1 {nops} | EMPTY;","title":"nops and nopse"},{"location":"disambiguation/#prefer","text":"This disambiguation filter is applicable to terminal productions only. It will be used to choose the right recognizer/terminal in case of lexical ambiguity . For example: INT: /[-+]?[0-9]+\\b/ {prefer}; FLOAT: /[-+]?[0-9]*\\.?[0-9]+([eE][-+]?[0-9]+)?\\b/; If in the grammar we have a possibility that both recognizers are tried, both will succeed for input 23 , but we want INT to be chosen in this case.","title":"prefer"},{"location":"disambiguation/#dynamic-disambiguation-filter","text":"All previously described filters are of static nature, i.e. they are compiled during LR table calculation (by removing erroneous automata transitions) and they don't depend on the parsed input. There are sometimes situations when parsing decision depends on the input. For example, lets say that we need to parse arithmetic expression but our operation priority increase for operations that are introduced later in the input. 1 + 2 * 3 - 4 + 5 Should be parsed as: ((1 + (2 * (3 - 4))) + 5) While 1 - 2 + 3 * 4 + 5 should be parsed as: (1 - ((2 + (3 * 4)) + 5)) As you can see, operations that appears later in the input are of higher priority. In parglare you can implement this dynamic behavior in two steps: First, mark productions in your grammar by dynamic rule: E: E op_sum E {dynamic} | E op_mul E {dynamic} | /\\d+/; op_sum: '+' {dynamic}; op_mul: '*' {dynamic}; This tells parglare that those production are candidates for dynamic ambiguity resolution. Second step is to register a predicate function, during parser construction, that will be used for resolution. This function operates as a filter for actions. It receives the parsing context for the action, the LR automata states between whom the transition is about to occur, and the sub-results in the case of a reduction. The function should return True if the transition is allowed or False otherwise. This function sometimes need to maintain some kind of state. To initialize its state at the beginning it is called with None as parameters. parser = Parser(grammar, dynamic_filter=custom_disambiguation_filter) Where resolution function is of the following form: def custom_disambiguation_filter(context, from_state, to_state, action, production, subresults): \"\"\" Make first operation that appears in the input as lower priority. This demonstrates how priority rule can change dynamically depending on the input. \"\"\" global operations # At the start of parsing this function is called with actions set to None # to give a chance for the strategy to initialize. if action is None: operations = [] return if action is SHIFT: operation = context.token.symbol else: operation = context.token_ahead.symbol actions = from_state.actions[operation] if operation not in operations and operation.name != 'STOP': operations.append(operation) if action is SHIFT: shifts = [a for a in actions if a.action is SHIFT] if not shifts: return False reductions = [a for a in actions if a.action is REDUCE] if not reductions: return True red_op = reductions[0].prod.rhs[1] return operations.index(operation) > operations.index(red_op) elif action is REDUCE: # Current reduction operation red_op = production.rhs[1] # If operation ahead is STOP or is of less or equal priority -> reduce. return ((operation not in operations) or (operations.index(operation) <= operations.index(red_op))) This function is a predicate that will be called for each action for productions marked with dynamic (SHIFT action for dynamic terminal production and REDUCE action for dynamic non-terminal productions). You are provided with enough information to make a custom decision whether to perform or reject the operation. Parameters are: context - the parsing context object . from_state/to_state -- LRState instances for the transition, action - either SHIFT or REDUCE constant from parglare module, production - a production used for the REDUCE operation. Valid only if action is REDUCE. subresults (list) - a sub-results for the reduction. Valid only for REDUCE. The length of this list is equal to len(production.rhs) . For details see test_dynamic_disambiguation_filters.py .","title":"Dynamic disambiguation filter"},{"location":"disambiguation/#disambiguation-of-a-glr-forest","text":"For GLR forests there is an additional option for a disambiguation. The GLR forest has a disambiguate method that accepts a callable of the following signature: def disam_callable(parent): It accepts the GLR GSS Parent object which has a list of possibilities and the callable should remove all invalid possibilities from the list. The callable is called bottom-up for all Parent object which have more than one possibility. For a full example with comments see this test .","title":"Disambiguation of a GLR Forest"},{"location":"disambiguation/#lexical-ambiguities","text":"There is another source of ambiguities. Parglare uses integrated scanner, thus tokens are determined on the fly. This gives greater lexical disambiguation power but lexical ambiguities might arise nevertheless. Lexical ambiguity is a situation when at some place in the input more than one recognizer match successfully. For example, if in the input we have 3.4 and we expect at this place either an integer or a float. Both of these recognizer can match the input. The integer recognizer would match 3 while the float recognizer would match 3.4 . What should we use? parglare has lexical disambiguation strategy that will use priorities first. If this fails (i.e. all terminals have the same priority) we continue with implicit disambiguation strategy as follows: String recognizers are preferred over regexes (i.e. the most specific match). If we still have multiple matches use longest-match strategy. If more recognizers still match use prefer rule if given. If all else fails raise an exception. In case of GLR, ambiguity will be handled by parser forking, i.e. you will end up with all solutions/trees. Thus, in terminal definition rules we can use priorities to favor some of the recognizers, or we can use prefer to favor recognizer if there are multiple matches of the same length. For example: number: /\\d+/ {15}; or: number: /\\d+/ {prefer}; Note Implicit lexical disambiguation is controlled by lexical_disambiguation parameter passed to Parser / GLRParser constructor. By default, Parser uses implict disambiguation while GLRParser doesn't. In addition, you can also specify that the terminal takes part in dynamic disambiguation: number: /\\d+/ {dynamic};","title":"Lexical ambiguities"},{"location":"disambiguation/#custom-token-recognition-and-lexical-disambiguation","text":"In the previous section a built-in parglare lexical disambiguation strategy is explained. There are use-cases when this strategy is not sufficient. For example, if we want to do fuzzy match of tokens and choose the most similar token at the position. parglare solves this problem by enabling you to implement a custom token recognition by registering a callable during parser instantiation that will, during parsing, get all the symbols expected at the current location and return a list of tokens (instances of Token class ) or None / empty list if no symbol is found at the location. This callable is registered during parser instantiation as the parameter custom_token_recognition . parser = Parser( grammar, custom_token_recognition=custom_token_recognition) The callable accepts: context - the parsing context object . get_tokens - a callable used to get the tokens recognized using the default strategy. Called without parameters. Custom disambiguation might decide to return this list if no change is necessary, reduce the list, or extend it with new tokens. See the example below how to return list with a token only if the default recognition doesn't succeed. Returns: a list of Token class instances or None /empty list if no token is found. To instantiate Token pass in the symbol and the value of the token. Value of the token is usually a sub-string of the input string. In the following test Bar and Baz non-terminals are fuzzy matched. The non-terminal with the higher score wins but only if the score is above 0.7. grammar = \"\"\" S: Element+; Element: Bar | Baz | Number; terminals Bar: /Bar. \\d+/; Baz: /Baz. \\d+/; Number: /\\d+/; \"\"\" g = Grammar.from_string(grammar) grammar = [g] def custom_token_recognition(context, get_tokens): \"\"\" Custom token recognition should return a single token that is recognized at the given place in the input string. \"\"\" # Call default token recognition. tokens = get_tokens() if tokens: # If default recognition succeeds use the result. return tokens else: # If no tokens are found do the fuzzy match. matchers = [ lambda x: difflib.SequenceMatcher(None, 'bar.', x.lower()), lambda x: difflib.SequenceMatcher(None, 'baz.', x.lower()) ] symbols = [ grammar[0].get_terminal('Bar'), grammar[0].get_terminal('Baz'), ] # Try to do fuzzy match at the position elem = context.input_str[context.position:context.position+4] elem_num = context.input_str[context.position:] number_matcher = re.compile('[^\\d]*(\\d+)') number_match = number_matcher.match(elem_num) ratios = [] for matcher in matchers: ratios.append(matcher(elem).ratio()) max_ratio_index = ratios.index(max(ratios)) if ratios[max_ratio_index] > 0.7 and number_match.group(1): return [Token(symbols[max_ratio_index], number_match.group())] parser = Parser( g, custom_token_recognition=custom_token_recognition) # Bar and Baz will be recognized by a fuzzy match result = parser.parse('bar. 56 Baz 12') assert result == ['bar. 56', 'Baz 12'] result = parser.parse('Buz. 34 bar 56') assert result == ['Buz. 34', 'bar 56'] result = parser.parse('Ba. 34 baz 56') assert result == ['Ba. 34', 'baz 56'] # But if Bar/Baz are too different from the correct pattern # we get ParseError. In this case `bza` score is below 0.7 # for both Bar and Baz symbols. with pytest.raises(ParseError): parser.parse('Bar. 34 bza 56') Note custom_token_recognition can be used to implement custom lexical disambiguation by calling get_tokens and then reducing returned list to a list with a single result. Tip See the end of the parse trees section for a tip on how to investigate ambiguities in GLR parsing.","title":"Custom token recognition and lexical disambiguation"},{"location":"getting_started/","text":"Getting started \u00b6 The first thing to do is to write your language grammar using the parglare grammar language . You write the grammar either as a Python string in your source code or as a separate file. In case you are writing a grammar of a complex language I would suggest the separate file approach. Although not mandatory, the convention is that parglare grammar files have .pg extension. The next step is to create the instance of the Grammar class. This is achieved by importing the Grammar class and calling either from_file or from_str methods supplying the file name for the former and the Python string for the later call. from parglare import Grammar file_name = ..... grammar = Grammar.from_file(file_name) If there is no errors in the grammar you now have the grammar instance. For more information see the section about Grammar class . Tip There is also a handy pglr command line tool that can be used for grammar checking, visualization and debugging. The next step is to create an instance of the parser. There are two options. If you want to use LR parser instantiate Parser class. For GLR instantiate GLRParser class. from parglare import Parser parser = Parser(grammar) or from parglare import GLRParser parser = GLRParser(grammar) You can provide additional parser parameters during instantiation. Note LR parser is faster as the GLR machinery brings a significant overhead. So, the general advice is to stick to the LR parsing until you are sure that you need additional power of GLR, i.e. either you need more than one token of lookahead or your language is inherently ambiguous. pglr tool will help you in investigating why you have LR conflicts in your grammar and there are some nice disambiguation features in parglare that will help you resolve some of those conflicts. Now parse your input calling parse method on the parser instance. result = parser.parse(input_str) Depending on whether you have configured actions and what parameters you used for parser instance you will get either: a nested lists if no actions are used, a parse tree if build_tree parser param is set to True , some other representation of your input if custom actions are used. In case of the GLR parser you will get a list of all possible results (a.k.a. the parse forest ). Where to go next? \u00b6 You can investigate various topics in the docs. The examples and the tests are also a good source of information.","title":"Getting started"},{"location":"getting_started/#getting-started","text":"The first thing to do is to write your language grammar using the parglare grammar language . You write the grammar either as a Python string in your source code or as a separate file. In case you are writing a grammar of a complex language I would suggest the separate file approach. Although not mandatory, the convention is that parglare grammar files have .pg extension. The next step is to create the instance of the Grammar class. This is achieved by importing the Grammar class and calling either from_file or from_str methods supplying the file name for the former and the Python string for the later call. from parglare import Grammar file_name = ..... grammar = Grammar.from_file(file_name) If there is no errors in the grammar you now have the grammar instance. For more information see the section about Grammar class . Tip There is also a handy pglr command line tool that can be used for grammar checking, visualization and debugging. The next step is to create an instance of the parser. There are two options. If you want to use LR parser instantiate Parser class. For GLR instantiate GLRParser class. from parglare import Parser parser = Parser(grammar) or from parglare import GLRParser parser = GLRParser(grammar) You can provide additional parser parameters during instantiation. Note LR parser is faster as the GLR machinery brings a significant overhead. So, the general advice is to stick to the LR parsing until you are sure that you need additional power of GLR, i.e. either you need more than one token of lookahead or your language is inherently ambiguous. pglr tool will help you in investigating why you have LR conflicts in your grammar and there are some nice disambiguation features in parglare that will help you resolve some of those conflicts. Now parse your input calling parse method on the parser instance. result = parser.parse(input_str) Depending on whether you have configured actions and what parameters you used for parser instance you will get either: a nested lists if no actions are used, a parse tree if build_tree parser param is set to True , some other representation of your input if custom actions are used. In case of the GLR parser you will get a list of all possible results (a.k.a. the parse forest ).","title":"Getting started"},{"location":"getting_started/#where-to-go-next","text":"You can investigate various topics in the docs. The examples and the tests are also a good source of information.","title":"Where to go next?"},{"location":"grammar/","text":"The grammar class and related APIs \u00b6 After you write your grammar either as a Python string or as a separate file the next step is to instantiate the grammar object that will be used to create the parser. There are two factory methods defined on the Grammar class for creating the Grammar instance: Grammar.from_string(grammar_string) - if your grammar is given as a Python string, Grammar.from_file(file_path) - if the grammar is given as a separate file. Both methods return initialized Grammar object that is passed as the first and the only mandatory parameter for the Parser/GLRParser constructor. Grammar factory methods additional parameters \u00b6 Both methods from_string and from_file accept additional optional parameters: recognizers - a dict of custom recognizers. These recognizers are mandatory if a non-textual content is being parsed and the grammar terminals don't provide recognizers. See recognizers section for more information. debug - set to True to put the grammar in debug/trace mode. False by default. See debugging section for more information. debug_parse - set to True to debug/trace grammar file/string parsing. False by default. debug_colors - set to True to enable terminal colors in debug/trace output. False by default. re_flags - regex flags used for regex recognizers. See Python re module. By default, flags are set to re.MULTILINE|re.VERBOSE . ignore_case - By default parsing is case sensitive. Set this param to True for case-insensitive parsing. Grammar class \u00b6 Attributes \u00b6 terminals - a dict of terminals (instances of Terminal ) keyed by fully qualified name; nonterminals - a dict of non-terminal (instances of NonTerminal ) keyed by fully qualified name; start_symbol - a grammar symbol of the start/root rule. By default this is the first rule in the grammar; productions - a list of productions ( Production instances); recognizers - a dict of user supplied recognizers keyed by the terminal rule name; classes - a dict of Python classes dynamically created for rules using named matches keyed by the rule name. Methods \u00b6 print_debug() - prints detailed debug/trace info; get_terminal(name) - gets the terminal by the given fully qualified name or None if not found; get_nonterminal(name) - gets the non-terminal by the given fully qualified name or None if not found; get_symbol(name) - gets either a terminal or non-terminal by the given fully qualified name or None if not found. GrammarSymbol class \u00b6 This is a base class for Terminal and NonTerminal . Attributes \u00b6 name - the name of the grammar symbol, fqn (property) - fully qualified name of the symbol. Qualified by import module names. location - an instance of parglare.common.Location . Gives information about location in the file (position and span). action_name - the action name assigned for the symbol. This is given in the grammar using the @ syntax . If action name is not provided in the grammar symbol name is used. action_fqn (property) - the fully qualiifed action name for the symbol. Qualified by the names of import modules. action - resolved reference to the action function given by the user using actions parameter of the parser. Overrides grammar action if provided. If not given will be the same as grammar_action . grammar_action - resolved reference to the action function specified in the grammar. Not used if action attribute is defined, i.e. action overrides grammar_action . Terminal class \u00b6 Attributes \u00b6 prior (int) - a priority used in disambiguation, recognizer (callable) - a callable in charge of recognition of this terminal in the input stream, prefer (bool) - If True this recognizer/terminal is preferred in case of conflict where multiple recognizer match at the same place and implicit disambiguation doesn't resolve the conflict. dynamic (bool) - True if disambiguation should be resolved dynamically . NonTerminal class \u00b6 Attributes \u00b6 productions (list) - A list of alternative productions for this non-terminal symbol. Production class \u00b6 Attributes \u00b6 symbol (GrammarSymbol) - LHS of the production, rhs (ProductionRHS) - RHS of this production, assignments (dict) - Assignment instances keyed by match name. Created by named matches , assoc (int) - associativity of the production. See parglare.grammar.ASSOC_{NONE|LEFT|RIGHT} prior (int) - integer defining priority of this production. Default priority is 10. dynamic (bool) - True if this production disambiguation should be resolved dynamically . prod_id (int) - ordinal number of the production in the grammar, prod_symbol_id - zero-based ordinal of the production for the symbol grammar symbol, i.e. the ordinal for the alternative choice for this symbol. ProductionRHS class \u00b6 Represents right hand side of the production. Inherits list and keeps symbols from the production but doesn't count nor returns by index EMPTY symbols in the production.","title":"Grammar class"},{"location":"grammar/#the-grammar-class-and-related-apis","text":"After you write your grammar either as a Python string or as a separate file the next step is to instantiate the grammar object that will be used to create the parser. There are two factory methods defined on the Grammar class for creating the Grammar instance: Grammar.from_string(grammar_string) - if your grammar is given as a Python string, Grammar.from_file(file_path) - if the grammar is given as a separate file. Both methods return initialized Grammar object that is passed as the first and the only mandatory parameter for the Parser/GLRParser constructor.","title":"The grammar class and related APIs"},{"location":"grammar/#grammar-factory-methods-additional-parameters","text":"Both methods from_string and from_file accept additional optional parameters: recognizers - a dict of custom recognizers. These recognizers are mandatory if a non-textual content is being parsed and the grammar terminals don't provide recognizers. See recognizers section for more information. debug - set to True to put the grammar in debug/trace mode. False by default. See debugging section for more information. debug_parse - set to True to debug/trace grammar file/string parsing. False by default. debug_colors - set to True to enable terminal colors in debug/trace output. False by default. re_flags - regex flags used for regex recognizers. See Python re module. By default, flags are set to re.MULTILINE|re.VERBOSE . ignore_case - By default parsing is case sensitive. Set this param to True for case-insensitive parsing.","title":"Grammar factory methods additional parameters"},{"location":"grammar/#grammar-class","text":"","title":"Grammar class"},{"location":"grammar/#attributes","text":"terminals - a dict of terminals (instances of Terminal ) keyed by fully qualified name; nonterminals - a dict of non-terminal (instances of NonTerminal ) keyed by fully qualified name; start_symbol - a grammar symbol of the start/root rule. By default this is the first rule in the grammar; productions - a list of productions ( Production instances); recognizers - a dict of user supplied recognizers keyed by the terminal rule name; classes - a dict of Python classes dynamically created for rules using named matches keyed by the rule name.","title":"Attributes"},{"location":"grammar/#methods","text":"print_debug() - prints detailed debug/trace info; get_terminal(name) - gets the terminal by the given fully qualified name or None if not found; get_nonterminal(name) - gets the non-terminal by the given fully qualified name or None if not found; get_symbol(name) - gets either a terminal or non-terminal by the given fully qualified name or None if not found.","title":"Methods"},{"location":"grammar/#grammarsymbol-class","text":"This is a base class for Terminal and NonTerminal .","title":"GrammarSymbol class"},{"location":"grammar/#attributes_1","text":"name - the name of the grammar symbol, fqn (property) - fully qualified name of the symbol. Qualified by import module names. location - an instance of parglare.common.Location . Gives information about location in the file (position and span). action_name - the action name assigned for the symbol. This is given in the grammar using the @ syntax . If action name is not provided in the grammar symbol name is used. action_fqn (property) - the fully qualiifed action name for the symbol. Qualified by the names of import modules. action - resolved reference to the action function given by the user using actions parameter of the parser. Overrides grammar action if provided. If not given will be the same as grammar_action . grammar_action - resolved reference to the action function specified in the grammar. Not used if action attribute is defined, i.e. action overrides grammar_action .","title":"Attributes"},{"location":"grammar/#terminal-class","text":"","title":"Terminal class"},{"location":"grammar/#attributes_2","text":"prior (int) - a priority used in disambiguation, recognizer (callable) - a callable in charge of recognition of this terminal in the input stream, prefer (bool) - If True this recognizer/terminal is preferred in case of conflict where multiple recognizer match at the same place and implicit disambiguation doesn't resolve the conflict. dynamic (bool) - True if disambiguation should be resolved dynamically .","title":"Attributes"},{"location":"grammar/#nonterminal-class","text":"","title":"NonTerminal class"},{"location":"grammar/#attributes_3","text":"productions (list) - A list of alternative productions for this non-terminal symbol.","title":"Attributes"},{"location":"grammar/#production-class","text":"","title":"Production class"},{"location":"grammar/#attributes_4","text":"symbol (GrammarSymbol) - LHS of the production, rhs (ProductionRHS) - RHS of this production, assignments (dict) - Assignment instances keyed by match name. Created by named matches , assoc (int) - associativity of the production. See parglare.grammar.ASSOC_{NONE|LEFT|RIGHT} prior (int) - integer defining priority of this production. Default priority is 10. dynamic (bool) - True if this production disambiguation should be resolved dynamically . prod_id (int) - ordinal number of the production in the grammar, prod_symbol_id - zero-based ordinal of the production for the symbol grammar symbol, i.e. the ordinal for the alternative choice for this symbol.","title":"Attributes"},{"location":"grammar/#productionrhs-class","text":"Represents right hand side of the production. Inherits list and keeps symbols from the production but doesn't count nor returns by index EMPTY symbols in the production.","title":"ProductionRHS class"},{"location":"grammar_language/","text":"The parglare grammar language \u00b6 The parglare grammar specification language is based on BNF with syntactic sugar extensions which are optional and builds on top of a pure BNF. parglare is based on Context-Free Grammars (CFGs) and a grammar is written declaratively. You don't have to think about the parsing process like in e.g. PEGs . Ambiguities are dealt with explicitly (see the section on conflicts ). Each grammar file consists of three parts: - zero or more imports of other grammar files. See grammar modularization - one or more derivation/production rules - zero or more terminal definitions Each derivation/production rule is of the form: <symbol>: <expression> ; where <symbol> is grammar non-terminal and <expression> is a sequence of terminals and non-terminals separated by choice operator | . For example: Fields: Field | Fields \",\" Field; Here Fields is a non-terminal grammar symbol and it is defined as either a single Field or, recursively, as Fields followed by a string terminal , and than by another Field . It is not given here but Field could also be defined as a non-terminal. For example: Field: QuotedField | FieldContent; Or it could be defined as a terminal in terminals section: terminals Field: /[A-Z]*/; This terminal definition uses regular expression recognizer. Terminals \u00b6 Terminal symbols of the grammar define the fundamental or atomic elements of your language -- tokens or lexemes (e.g. keywords, numbers). In parglare a terminal is connected to the recognizer which is an object used to recognize token of a particular type in the input. Most of the time you will do parsing of textual content and you will need textual recognizers. These recognizers are built-in and there are two type of textual recognizers: string recognizer regular expression recognizer Terminals are given at the end of the grammar file, after production rules, following the keyword terminals . String recognizer \u00b6 String recognizer is defined as a plain string inside of double quotes: my_rule: \"start\" other_rule \"end\"; In this example \"start\" and \"end\" will be terminals with string recognizers that match exactly the words start and end . You can write string recognizing terminal directly in the rule expression or you can define terminal separately and reference it by name, like: my_rule: start other_rule end; terminals start: \"start\"; end: \"end\"; Either way it will be the same terminal. You can't mix those two approaches for a single terminal. If you defined a terminal in the terminals section than you can't use inline string matches for that terminal. You will usually write it as a separate terminal if the terminal is used at multiple places in the grammar or to provide disambiguation information for a terminal (priority, prefer etc.). Regular expression recognizer \u00b6 Or regex recognizer for short is a regex pattern written inside slashes ( /.../ ). For example: number: /\\d+/; This rule defines terminal symbol number which has a regex recognizer and will recognize one or more digits as a number. Note You cannot write regex recognizers inline like you can do with string recognizers. This constraint is introduced because there is no sane way to deduce terminal name given its regex. Thus, you must write all regex recognizers/terminals in the terminals section at the end of the grammar file. Custom recognizers \u00b6 If you are parsing arbitrary input (non-textual) you'll have to provide your own recognizers. In the grammar, you just have to provide terminal symbol without body, i.e. without string or regex recognizer. You will provide missing recognizers during grammar instantiation from Python. Although you don't supply body of the terminal you can define disambiguation rules as usual. Lets say that we have a list of integers (real list of Python ints, not a text with numbers) and we have some weird requirement to break those numbers according to the following grammar: Numbers: all_less_than_five ascending all_less_than_five; all_less_than_five: all_less_than_five int_less_than_five | int_less_than_five; terminals // These terminals have no recognizers defined in the grammar ascending: ; int_less_than_five: ; So, we should first match all numbers less than five and collect those, than we should match a list of ascending numbers and than list of less than five again. int_less_than_five and ascending are terminals/recognizers that will be defined in Python and passed to grammar construction. int_less_than_five will recognize Python integer that is, well, less than five. ascending will recognize a sublist of integers in ascending order. For more details on the usage see this test . More on this topic can be found in a separate section . Usual patterns \u00b6 This section explains how some common grammar patterns can be written using just a plain BNF notation. One or more \u00b6 // sections rule below will match one or more section. sections: sections section | section; In this example sections will match one or more section . Notice the recursive definition of the rule. You can read this as sections is either a single section or sections and a section . Note Please note that you could do the same with this rule: sections: section sections | section; which will give you similar result but the resulting tree will be different. Notice the recursive reference is now at the and of the first production. Previous example will reduce sections early and than add another section to it, thus the tree will be expanding to the left. The example in this note will collect all the sections and than start reducing from the end, thus building a tree expanding to the right. These are subtle differences that are important when you start writing your semantic actions. Most of the time you don't care about this so use the first version as it is more efficient and parglare provides built-in actions for these common cases. Zero or more \u00b6 // sections rule below will match zero or more section. sections: sections section | section | EMPTY; In this example sections will match zero or more section . Notice the addition of the EMPTY choice at the end. This means that matching nothing is a valid sections non-terminal. Same note from above applies here to. Optional \u00b6 document: optheader body; optheader: header | EMPTY; In this example optheader is either a header or nothing. Syntactic sugar - BNF extensions \u00b6 Previous section gives the overview of the basic BNF syntax. If you got to use various BNF extensions (like Kleene star ) you might find writing patterns in the previous section awkward. Since some of the patterns are used frequently in the grammars (zero-or-more, one-or-more etc.) parglare provides syntactic sugar for this common idioms using a well known regular expression syntax. Optional \u00b6 Optional can be specified using ? . For example: S: \"2\" b? \"3\"?; terminals b: \"1\"; Here, after 2 we might have terminal b but it is optional, as well as 3 that follows. Lets see what the parser will return for various inputs (the grammar variable is a string holding grammar from above): g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 1 3' result = p.parse(input_str) assert result == [\"2\", \"1\", \"3\"] input_str = '2 3' result = p.parse(input_str) assert result == [\"2\", None, \"3\"] Note Syntax equivalence for optional operator: S: b?; terminals b: \"1\"; is equivalent to: S: b_opt; b_opt: b | EMPTY; terminals b: \"1\"; Behind the scenes parglare will create b_opt rule. All syntactic sugar additions operate by creating additional rules in the grammar during table construction. One or more \u00b6 One or more match is specified using + operator. For example: S: \"2\" c+; terminals c: \"c\"; After 2 we expect to see one or more c terminals. Lets see what the parser will return for various inputs (the grammar variable is a string holding grammar from above): g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 c c c' result = p.parse(input_str) assert result == [\"2\", [\"c\", \"c\", \"c\"]] input_str = '2 c' result = p.parse(input_str) assert result == [\"2\", [\"c\"]] So the sub-expression on the second position ( c+ sub-rule) will by default produce a list of matched c terminals. If c is missing a parse error will be raised. Note Syntax equivalence for one or more : S: a+; terminals a: \"a\"; is equivalent to: S: a_1; @collect a_1: a_1 a | a; terminals a: \"a\"; + operator allows repetition modifier for separators. For example: S: \"2\" c+[comma]; terminals c: \"c\"; comma: \",\"; c+[comma] will match one or more c terminals separated by whatever is matched by the comma rule. Lets see what the parser will return for various inputs (the grammar variable is a string holding grammar from above): g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 c, c, c' result = p.parse(input_str) assert result == [\"2\", [\"c\", \"c\", \"c\"]] input_str = '2 c' result = p.parse(input_str) assert result == [\"2\", [\"c\"]] As you can see giving a separator modifier allows us to parse a list of items separated by the whatever is matched by the rule given inside [] . Note Syntax equivalence one or more with separator : S: a+[comma]; terminals a: \"a\"; comma: \",\"; is equivalent to: S: a_1_comma; @collect_sep a_1_comma: a_1_comma comma a | a; terminals a: \"a\"; comma: \",\"; Making the name of the separator rule a suffix of the additional rule name makes sure that only one additional rule will be added to the grammar for all instances of a+[comma] , i.e. same base rule with the same separator. Zero or more \u00b6 Zero or more match is specified using * operator. For example: S: \"2\" c*; terminals c: \"c\"; This syntactic addition is similar to + except that it doesn't require rule to match at least once. If there is no match, resulting sub-expression will be an empty list. For example: g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 c c c' result = p.parse(input_str) assert result == [\"2\", [\"c\", \"c\", \"c\"]] input_str = '2' result = p.parse(input_str) assert result == [\"2\", []] Note Syntax equivalence zero or more : S: a*; terminals a: \"a\"; is equivalent to: S: a_0; a_0: a_1 {nops} | EMPTY; @collect a_1: a_1 a | a; terminals a: \"a\"; So using of * creates both a_0 and a_1 rules. Action attached to a_0 returns a list of matched a and empty list if no match is found. Please note the usage of nops . In case if prefer_shift strategy is used using nops will perform both REDUCE and SHIFT during GLR parsing in case what follows zero or more might be another element in the sequence. This is most of the time what you need. Same as one or more this operator may use separator modifiers. Note Syntax equivalence zero or more with separator : S: a*[comma]; terminals a: \"a\"; comma: \",\"; is equivalent to: S: a_0_comma; a_0_comma: a_1_comma {nops} | EMPTY; @collect_sep a_1_comma: a_1_comma comma a | a; terminals a: \"a\"; where action is attached to a_0_comma to provide returning a list of matched a and empty list if no match is found. Greedy repetitions \u00b6 * , + , and ? operators have their greedy counterparts. To make an repetition operator greedy add ! (e.g. *! , +! , and ?! ). These versions will consume as much as possible before proceeding. You can think of the greedy repetitions as a way to disambiguate a class of ambiguities which arises due to a sequence of rules where earlier constituent can match an input of various length leaving the rest to the next rule to consume. Consider this example: S: \"a\"* \"a\"*; It is easy to see that this grammar is ambiguous, as for the input: a a We have 3 solutions: 1:S[0->3] a_0[0->1] a_1[0->1] a[0->1, \"a\"] a_0[2->3] a_1[2->3] a[2->3, \"a\"] 2:S[0->3] a_0[0->0] a_0[0->3] a_1[0->3] a_1[0->1] a[0->1, \"a\"] a[2->3, \"a\"] 3:S[0->3] a_0[0->3] a_1[0->3] a_1[0->1] a[0->1, \"a\"] a[2->3, \"a\"] a_0[3->3] If we apply greedy zero-or-more to the first element of the sequence: S: \"a\"*! \"a\"*; We have only one solution where all a tokens are consumed by the first part of the rule: S[0->3] a_0[0->3] a_1[0->3] a_1[0->1] a[0->1, \"a\"] a[2->3, \"a\"] a_0[3->3] Parenthesized groups \u00b6 You can use parenthesized groups at any place you can use a rule reference. For example: S: a (b* a {left} | b); terminals a: \"a\"; b: \"b\"; Here, you can see that S will match a and then either b* a or b . You can also see that meta-data can be applied at a per-sequence level (in this case {left} applies to sequence b* a ). Here is a more complex example which uses repetitions, separators, assignments and nested groups. S: (b c)*[comma]; S: (b c)*[comma] a=(a+ (b | c)*)+[comma]; terminals a: \"a\"; b: \"b\"; c: \"c\"; comma: \",\"; Note Syntax equivalence parenthesized groups : S: c (b* c {left} | b); terminals c: \"c\"; b: \"b\"; is equivalent to: S: c S_g1; S_g1: b_0 c {left} | b; b_0: b_1 | EMPTY; b_1: b_1 b | b; terminals c: \"c\"; b: \"b\"; So using parenthesized groups creates additional _g<n> rules ( S_g1 in the example), where n is a unique number per rule starting from 1 . All other syntactic sugar elements applied to groups behave as expected. EMPTY built-in rule \u00b6 There is a special EMPTY rule you can reference in your grammars. EMPTY rule will reduce without consuming any input and will always succeed, i.e. it is empty recognition. Named matches ( assignments ) \u00b6 In section on actions you can see that semantic action (Python callable) connected to a rule will be called with two parameters: a context and a list of sub-expressions evaluation results. This require you to use positional access in the list of sub-expressions. Named matches (a.k.a assignments ) enable giving a name to the sub-expression directly in the grammar. For example: S: first=a second=digit+[comma]; terminals a: \"a\"; digit: /\\d+/; comma: \",\"; In this example root rule matches one a and then one or more digit separated by a comma. You can see that the first sub-expression ( a match) is assigned to first while the second sub-expression digit+[comma] is assigned to second . first and second will now be an additional keyword parameters passed to the semantic action. The values passed in using these parameters will be the results of evaluation of the rules referenced by the assignments. There are two kind of assignments: plain assignment ( = ) -- will collect RHS and pass it to the action under the names given by LHS, bool assignment ( ?= ) -- will pass True if the match return non-empty result. If the result of RHS is empty the assignment will result in False being passed to the action. Each rule using named matches result in a dynamically created Python class named after the rule. These classes are kept in a dictionary grammar.classes and used to instantiate Python objects during parsing by an implicitly set built-in obj action . Thus, for rules using named matches, default action is to create object with attributes whose names are those of LHS of the assignments and values are from RHS of the assignments (or boolean values for bool assignments). Each object is an instance of corresponding dynamically created Python class. Effectively, using named matches enables automatic creation of a nice AST. Tip You can, of course, override default action either in the grammar using @ syntax or using actions dict given to the parser. See the next section. Referencing semantic actions from a grammar \u00b6 By default action with the name same as the rule name will be searched in the accompanying <grammar>_actions.py file or actions dict . You can override this by specifying action name for the rule directly in the grammar using @ syntax. In that case a name given after @ will be used instead of a rule name. For example: @myaction some_rule: first second; For rule some_rule action with the name myaction will be searched in the <grammar>_actions.py module, actions dict or built-in actions provided by the parglare.actions module. This is helpful if you have some common action that can be used for multiple rules in your grammar. Also this can be used to specify built-in action to be used for a rule directly in the grammar. User meta-data \u00b6 You can supply arbitrary meta-data for the productions and terminals in the grammar in the form of key-value pairs. This can be used to augment dynamic disambiguation strategies, error reporting etc. To define meta-data put it inside the {} block of either rule, production or terminal in the form of name: value , where name is a valid ID and value is integer, float, bool ( true or false ) or string in single quotes. For example: grammar_str = r''' MyRule: 'a' {left, 1, dynamic, nops, some_string:'My Label', some_bool: true, some_int: 3, some_float: 4.5}; ''' grammar = Grammar.from_string(grammar_str) my_rule = grammar.get_nonterminal('MyRule') prod = my_rule.productions[0] assert prod.some_string == 'My Label' assert prod.some_bool is True assert prod.some_int == 3 assert prod.some_float == 4.5 In this example, user meta-data some_string with value My Label is defined on the first production of rule MyRule . Please note that user defined meta-data is accessed as an ordinary Python attribute. In the example you can also see the definition of meta-data of various supported types. User meta-data can be defined at the rule level in which case all production for the given rule inherit the meta-data. For example: grammar_str = r''' MyRule {label: 'My Label', nops}: 'a' {left, 1, dynamic}; ''' grammar = Grammar.from_string(grammar_str) my_rule = grammar.get_nonterminal('MyRule') # User meta-data is accessible on the non-terminal assert my_rule.label == 'My Label' # The production has its own meta-data prod = my_rule.productions[0] assert prod.assoc == ASSOC_LEFT assert prod.prior == 1 assert prod.dynamic # Rule-level meta-data are propagated to productions assert prod.label == 'My Label' Meta-data defined on the rule level can be overridden on the production level. Also, rule can be specified multiple times. Propagation of each rule meta-data is done only to the productions specified in the rule. For example: grammar_str = r''' MyRule {label: 'My Label', left}: 'first' {right, label: 'My overriden label'} | 'second'; MyRule {label: 'Other rule'}: 'third' {left} | 'fourth' {label: 'Fourth prod'}; ''' grammar = Grammar.from_string(grammar_str) my_rule = grammar.get_nonterminal('MyRule') # User meta-data is accessible on the non-terminal # Rule level meta-data are only those defined on the # first rule in the order of the definition. assert my_rule.label == 'My Label' prod1 = my_rule.productions[0] # First production overrides meta-data assert prod1.label == 'My overriden label' assert prod1.assoc == ASSOC_RIGHT # If not overriden it uses meta-data from the rule. prod2 = my_rule.productions[1] assert prod2.label == 'My Label' assert prod2.assoc == ASSOC_LEFT # Third and fourth production belongs to the second rule so they # inherits its meta-data. prod3 = my_rule.productions[2] assert prod3.label == 'Other rule' assert prod3.assoc == ASSOC_LEFT prod4 = my_rule.productions[3] assert prod4.label == 'Fourth prod' assert prod4.assoc == ASSOC_NONE Grammar comments \u00b6 In parglare grammar, comments are available as both line comments and block comments: // This is a line comment. Everything from the '//' to the end of line is a comment. /* This is a block comment. Everything in between `/*` and '*/' is a comment. */ Handling whitespaces and comments in your language \u00b6 By default parser will skip whitespaces. Whitespace skipping is controlled by ws parameter to the parser which is by default set to '\\n\\t ' . If you need more control of the layout, i.e. handling of not only whitespaces but comments also, you can use a special rule LAYOUT : LAYOUT: LayoutItem | LAYOUT LayoutItem | EMPTY; LayoutItem: WS | Comment; terminals WS: /\\s+/; Comment: /\\/\\/.*/; This will form a separate layout parser that will parse in-between each matched tokens. In this example whitespaces and line-comments will be consumed by the layout parser. If this special rule is found in the grammar ws parser parameter is ignored. Here is another example that gives support for both line comments and block comments like the one used in the grammar language itself: LAYOUT: LayoutItem | LAYOUT LayoutItem | EMPTY; LayoutItem: WS | Comment; Comment: '/*' CorNCs '*/' | LineComment; CorNCs: CorNC | CorNCs CorNC | EMPTY; CorNC: Comment | NotComment | WS; terminals WS: /\\s+/; LineComment: /\\/\\/.*/; NotComment: /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/; Tip If LAYOUT is provided it must match before the first token, between any two tokens in the input, and after the last token. If layout cannot be empty, the input cannot start or end with a token. If this is not desired, make sure to include EMPTY in the layout as one of its alternatives like in the previous examples. Handling keywords in your language \u00b6 By default parser will match given string recognizer even if it is part of some larger word, i.e. it will not require matching on the word boundary. This is not the desired behavior for language keywords. For example, lets examine this little grammar: S: \"for\" name=ID \"=\" from=INT \"to\" to=INT; terminals ID: /\\w+/; INT: /\\d+/; This grammar is intended to match statement like this one: for a=10 to 20 But it will also match: fora=10 to20 which is not what we wanted. parglare allows the definition of a special terminal rule KEYWORD . This rule must define a regular expression recognizer . Any string recognizer in the grammar that can be also recognized by the KEYWORD recognizer is treated as a keyword and is changed during grammar construction to match only on word boundary. For example: S: \"for\" name=ID \"=\" from=INT \"to\" to=INT; terminals ID: /\\w+/; INT: /\\d+/; KEYWORD: /\\w+/; Now, fora=10 to20 will not be recognized as the words for and to are recognized to be keywords (they can be matched by the KEYWORD rule). This will be parsed correctly: for a=10 to 20 As = is not matched by the KEYWORD rule and thus doesn't require to be separated from the surrounding tokens. Note parglare uses integrated scanner so this example: for for=10 to 20 will be correctly parsed. for in for=10 will be recognized as ID and not as a keyword for , i.e. there is no lexical ambiguity due to tokenizer separation.","title":"Grammar language"},{"location":"grammar_language/#the-parglare-grammar-language","text":"The parglare grammar specification language is based on BNF with syntactic sugar extensions which are optional and builds on top of a pure BNF. parglare is based on Context-Free Grammars (CFGs) and a grammar is written declaratively. You don't have to think about the parsing process like in e.g. PEGs . Ambiguities are dealt with explicitly (see the section on conflicts ). Each grammar file consists of three parts: - zero or more imports of other grammar files. See grammar modularization - one or more derivation/production rules - zero or more terminal definitions Each derivation/production rule is of the form: <symbol>: <expression> ; where <symbol> is grammar non-terminal and <expression> is a sequence of terminals and non-terminals separated by choice operator | . For example: Fields: Field | Fields \",\" Field; Here Fields is a non-terminal grammar symbol and it is defined as either a single Field or, recursively, as Fields followed by a string terminal , and than by another Field . It is not given here but Field could also be defined as a non-terminal. For example: Field: QuotedField | FieldContent; Or it could be defined as a terminal in terminals section: terminals Field: /[A-Z]*/; This terminal definition uses regular expression recognizer.","title":"The parglare grammar language"},{"location":"grammar_language/#terminals","text":"Terminal symbols of the grammar define the fundamental or atomic elements of your language -- tokens or lexemes (e.g. keywords, numbers). In parglare a terminal is connected to the recognizer which is an object used to recognize token of a particular type in the input. Most of the time you will do parsing of textual content and you will need textual recognizers. These recognizers are built-in and there are two type of textual recognizers: string recognizer regular expression recognizer Terminals are given at the end of the grammar file, after production rules, following the keyword terminals .","title":"Terminals"},{"location":"grammar_language/#string-recognizer","text":"String recognizer is defined as a plain string inside of double quotes: my_rule: \"start\" other_rule \"end\"; In this example \"start\" and \"end\" will be terminals with string recognizers that match exactly the words start and end . You can write string recognizing terminal directly in the rule expression or you can define terminal separately and reference it by name, like: my_rule: start other_rule end; terminals start: \"start\"; end: \"end\"; Either way it will be the same terminal. You can't mix those two approaches for a single terminal. If you defined a terminal in the terminals section than you can't use inline string matches for that terminal. You will usually write it as a separate terminal if the terminal is used at multiple places in the grammar or to provide disambiguation information for a terminal (priority, prefer etc.).","title":"String recognizer"},{"location":"grammar_language/#regular-expression-recognizer","text":"Or regex recognizer for short is a regex pattern written inside slashes ( /.../ ). For example: number: /\\d+/; This rule defines terminal symbol number which has a regex recognizer and will recognize one or more digits as a number. Note You cannot write regex recognizers inline like you can do with string recognizers. This constraint is introduced because there is no sane way to deduce terminal name given its regex. Thus, you must write all regex recognizers/terminals in the terminals section at the end of the grammar file.","title":"Regular expression recognizer"},{"location":"grammar_language/#custom-recognizers","text":"If you are parsing arbitrary input (non-textual) you'll have to provide your own recognizers. In the grammar, you just have to provide terminal symbol without body, i.e. without string or regex recognizer. You will provide missing recognizers during grammar instantiation from Python. Although you don't supply body of the terminal you can define disambiguation rules as usual. Lets say that we have a list of integers (real list of Python ints, not a text with numbers) and we have some weird requirement to break those numbers according to the following grammar: Numbers: all_less_than_five ascending all_less_than_five; all_less_than_five: all_less_than_five int_less_than_five | int_less_than_five; terminals // These terminals have no recognizers defined in the grammar ascending: ; int_less_than_five: ; So, we should first match all numbers less than five and collect those, than we should match a list of ascending numbers and than list of less than five again. int_less_than_five and ascending are terminals/recognizers that will be defined in Python and passed to grammar construction. int_less_than_five will recognize Python integer that is, well, less than five. ascending will recognize a sublist of integers in ascending order. For more details on the usage see this test . More on this topic can be found in a separate section .","title":"Custom recognizers"},{"location":"grammar_language/#usual-patterns","text":"This section explains how some common grammar patterns can be written using just a plain BNF notation.","title":"Usual patterns"},{"location":"grammar_language/#one-or-more","text":"// sections rule below will match one or more section. sections: sections section | section; In this example sections will match one or more section . Notice the recursive definition of the rule. You can read this as sections is either a single section or sections and a section . Note Please note that you could do the same with this rule: sections: section sections | section; which will give you similar result but the resulting tree will be different. Notice the recursive reference is now at the and of the first production. Previous example will reduce sections early and than add another section to it, thus the tree will be expanding to the left. The example in this note will collect all the sections and than start reducing from the end, thus building a tree expanding to the right. These are subtle differences that are important when you start writing your semantic actions. Most of the time you don't care about this so use the first version as it is more efficient and parglare provides built-in actions for these common cases.","title":"One or more"},{"location":"grammar_language/#zero-or-more","text":"// sections rule below will match zero or more section. sections: sections section | section | EMPTY; In this example sections will match zero or more section . Notice the addition of the EMPTY choice at the end. This means that matching nothing is a valid sections non-terminal. Same note from above applies here to.","title":"Zero or more"},{"location":"grammar_language/#optional","text":"document: optheader body; optheader: header | EMPTY; In this example optheader is either a header or nothing.","title":"Optional"},{"location":"grammar_language/#syntactic-sugar-bnf-extensions","text":"Previous section gives the overview of the basic BNF syntax. If you got to use various BNF extensions (like Kleene star ) you might find writing patterns in the previous section awkward. Since some of the patterns are used frequently in the grammars (zero-or-more, one-or-more etc.) parglare provides syntactic sugar for this common idioms using a well known regular expression syntax.","title":"Syntactic sugar - BNF extensions"},{"location":"grammar_language/#optional_1","text":"Optional can be specified using ? . For example: S: \"2\" b? \"3\"?; terminals b: \"1\"; Here, after 2 we might have terminal b but it is optional, as well as 3 that follows. Lets see what the parser will return for various inputs (the grammar variable is a string holding grammar from above): g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 1 3' result = p.parse(input_str) assert result == [\"2\", \"1\", \"3\"] input_str = '2 3' result = p.parse(input_str) assert result == [\"2\", None, \"3\"] Note Syntax equivalence for optional operator: S: b?; terminals b: \"1\"; is equivalent to: S: b_opt; b_opt: b | EMPTY; terminals b: \"1\"; Behind the scenes parglare will create b_opt rule. All syntactic sugar additions operate by creating additional rules in the grammar during table construction.","title":"Optional"},{"location":"grammar_language/#one-or-more_1","text":"One or more match is specified using + operator. For example: S: \"2\" c+; terminals c: \"c\"; After 2 we expect to see one or more c terminals. Lets see what the parser will return for various inputs (the grammar variable is a string holding grammar from above): g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 c c c' result = p.parse(input_str) assert result == [\"2\", [\"c\", \"c\", \"c\"]] input_str = '2 c' result = p.parse(input_str) assert result == [\"2\", [\"c\"]] So the sub-expression on the second position ( c+ sub-rule) will by default produce a list of matched c terminals. If c is missing a parse error will be raised. Note Syntax equivalence for one or more : S: a+; terminals a: \"a\"; is equivalent to: S: a_1; @collect a_1: a_1 a | a; terminals a: \"a\"; + operator allows repetition modifier for separators. For example: S: \"2\" c+[comma]; terminals c: \"c\"; comma: \",\"; c+[comma] will match one or more c terminals separated by whatever is matched by the comma rule. Lets see what the parser will return for various inputs (the grammar variable is a string holding grammar from above): g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 c, c, c' result = p.parse(input_str) assert result == [\"2\", [\"c\", \"c\", \"c\"]] input_str = '2 c' result = p.parse(input_str) assert result == [\"2\", [\"c\"]] As you can see giving a separator modifier allows us to parse a list of items separated by the whatever is matched by the rule given inside [] . Note Syntax equivalence one or more with separator : S: a+[comma]; terminals a: \"a\"; comma: \",\"; is equivalent to: S: a_1_comma; @collect_sep a_1_comma: a_1_comma comma a | a; terminals a: \"a\"; comma: \",\"; Making the name of the separator rule a suffix of the additional rule name makes sure that only one additional rule will be added to the grammar for all instances of a+[comma] , i.e. same base rule with the same separator.","title":"One or more"},{"location":"grammar_language/#zero-or-more_1","text":"Zero or more match is specified using * operator. For example: S: \"2\" c*; terminals c: \"c\"; This syntactic addition is similar to + except that it doesn't require rule to match at least once. If there is no match, resulting sub-expression will be an empty list. For example: g = Grammar.from_string(grammar) p = Parser(g) input_str = '2 c c c' result = p.parse(input_str) assert result == [\"2\", [\"c\", \"c\", \"c\"]] input_str = '2' result = p.parse(input_str) assert result == [\"2\", []] Note Syntax equivalence zero or more : S: a*; terminals a: \"a\"; is equivalent to: S: a_0; a_0: a_1 {nops} | EMPTY; @collect a_1: a_1 a | a; terminals a: \"a\"; So using of * creates both a_0 and a_1 rules. Action attached to a_0 returns a list of matched a and empty list if no match is found. Please note the usage of nops . In case if prefer_shift strategy is used using nops will perform both REDUCE and SHIFT during GLR parsing in case what follows zero or more might be another element in the sequence. This is most of the time what you need. Same as one or more this operator may use separator modifiers. Note Syntax equivalence zero or more with separator : S: a*[comma]; terminals a: \"a\"; comma: \",\"; is equivalent to: S: a_0_comma; a_0_comma: a_1_comma {nops} | EMPTY; @collect_sep a_1_comma: a_1_comma comma a | a; terminals a: \"a\"; where action is attached to a_0_comma to provide returning a list of matched a and empty list if no match is found.","title":"Zero or more"},{"location":"grammar_language/#greedy-repetitions","text":"* , + , and ? operators have their greedy counterparts. To make an repetition operator greedy add ! (e.g. *! , +! , and ?! ). These versions will consume as much as possible before proceeding. You can think of the greedy repetitions as a way to disambiguate a class of ambiguities which arises due to a sequence of rules where earlier constituent can match an input of various length leaving the rest to the next rule to consume. Consider this example: S: \"a\"* \"a\"*; It is easy to see that this grammar is ambiguous, as for the input: a a We have 3 solutions: 1:S[0->3] a_0[0->1] a_1[0->1] a[0->1, \"a\"] a_0[2->3] a_1[2->3] a[2->3, \"a\"] 2:S[0->3] a_0[0->0] a_0[0->3] a_1[0->3] a_1[0->1] a[0->1, \"a\"] a[2->3, \"a\"] 3:S[0->3] a_0[0->3] a_1[0->3] a_1[0->1] a[0->1, \"a\"] a[2->3, \"a\"] a_0[3->3] If we apply greedy zero-or-more to the first element of the sequence: S: \"a\"*! \"a\"*; We have only one solution where all a tokens are consumed by the first part of the rule: S[0->3] a_0[0->3] a_1[0->3] a_1[0->1] a[0->1, \"a\"] a[2->3, \"a\"] a_0[3->3]","title":"Greedy repetitions"},{"location":"grammar_language/#parenthesized-groups","text":"You can use parenthesized groups at any place you can use a rule reference. For example: S: a (b* a {left} | b); terminals a: \"a\"; b: \"b\"; Here, you can see that S will match a and then either b* a or b . You can also see that meta-data can be applied at a per-sequence level (in this case {left} applies to sequence b* a ). Here is a more complex example which uses repetitions, separators, assignments and nested groups. S: (b c)*[comma]; S: (b c)*[comma] a=(a+ (b | c)*)+[comma]; terminals a: \"a\"; b: \"b\"; c: \"c\"; comma: \",\"; Note Syntax equivalence parenthesized groups : S: c (b* c {left} | b); terminals c: \"c\"; b: \"b\"; is equivalent to: S: c S_g1; S_g1: b_0 c {left} | b; b_0: b_1 | EMPTY; b_1: b_1 b | b; terminals c: \"c\"; b: \"b\"; So using parenthesized groups creates additional _g<n> rules ( S_g1 in the example), where n is a unique number per rule starting from 1 . All other syntactic sugar elements applied to groups behave as expected.","title":"Parenthesized groups"},{"location":"grammar_language/#empty-built-in-rule","text":"There is a special EMPTY rule you can reference in your grammars. EMPTY rule will reduce without consuming any input and will always succeed, i.e. it is empty recognition.","title":"EMPTY built-in rule"},{"location":"grammar_language/#named-matches-assignments","text":"In section on actions you can see that semantic action (Python callable) connected to a rule will be called with two parameters: a context and a list of sub-expressions evaluation results. This require you to use positional access in the list of sub-expressions. Named matches (a.k.a assignments ) enable giving a name to the sub-expression directly in the grammar. For example: S: first=a second=digit+[comma]; terminals a: \"a\"; digit: /\\d+/; comma: \",\"; In this example root rule matches one a and then one or more digit separated by a comma. You can see that the first sub-expression ( a match) is assigned to first while the second sub-expression digit+[comma] is assigned to second . first and second will now be an additional keyword parameters passed to the semantic action. The values passed in using these parameters will be the results of evaluation of the rules referenced by the assignments. There are two kind of assignments: plain assignment ( = ) -- will collect RHS and pass it to the action under the names given by LHS, bool assignment ( ?= ) -- will pass True if the match return non-empty result. If the result of RHS is empty the assignment will result in False being passed to the action. Each rule using named matches result in a dynamically created Python class named after the rule. These classes are kept in a dictionary grammar.classes and used to instantiate Python objects during parsing by an implicitly set built-in obj action . Thus, for rules using named matches, default action is to create object with attributes whose names are those of LHS of the assignments and values are from RHS of the assignments (or boolean values for bool assignments). Each object is an instance of corresponding dynamically created Python class. Effectively, using named matches enables automatic creation of a nice AST. Tip You can, of course, override default action either in the grammar using @ syntax or using actions dict given to the parser. See the next section.","title":"Named matches (assignments)"},{"location":"grammar_language/#referencing-semantic-actions-from-a-grammar","text":"By default action with the name same as the rule name will be searched in the accompanying <grammar>_actions.py file or actions dict . You can override this by specifying action name for the rule directly in the grammar using @ syntax. In that case a name given after @ will be used instead of a rule name. For example: @myaction some_rule: first second; For rule some_rule action with the name myaction will be searched in the <grammar>_actions.py module, actions dict or built-in actions provided by the parglare.actions module. This is helpful if you have some common action that can be used for multiple rules in your grammar. Also this can be used to specify built-in action to be used for a rule directly in the grammar.","title":"Referencing semantic actions from a grammar"},{"location":"grammar_language/#user-meta-data","text":"You can supply arbitrary meta-data for the productions and terminals in the grammar in the form of key-value pairs. This can be used to augment dynamic disambiguation strategies, error reporting etc. To define meta-data put it inside the {} block of either rule, production or terminal in the form of name: value , where name is a valid ID and value is integer, float, bool ( true or false ) or string in single quotes. For example: grammar_str = r''' MyRule: 'a' {left, 1, dynamic, nops, some_string:'My Label', some_bool: true, some_int: 3, some_float: 4.5}; ''' grammar = Grammar.from_string(grammar_str) my_rule = grammar.get_nonterminal('MyRule') prod = my_rule.productions[0] assert prod.some_string == 'My Label' assert prod.some_bool is True assert prod.some_int == 3 assert prod.some_float == 4.5 In this example, user meta-data some_string with value My Label is defined on the first production of rule MyRule . Please note that user defined meta-data is accessed as an ordinary Python attribute. In the example you can also see the definition of meta-data of various supported types. User meta-data can be defined at the rule level in which case all production for the given rule inherit the meta-data. For example: grammar_str = r''' MyRule {label: 'My Label', nops}: 'a' {left, 1, dynamic}; ''' grammar = Grammar.from_string(grammar_str) my_rule = grammar.get_nonterminal('MyRule') # User meta-data is accessible on the non-terminal assert my_rule.label == 'My Label' # The production has its own meta-data prod = my_rule.productions[0] assert prod.assoc == ASSOC_LEFT assert prod.prior == 1 assert prod.dynamic # Rule-level meta-data are propagated to productions assert prod.label == 'My Label' Meta-data defined on the rule level can be overridden on the production level. Also, rule can be specified multiple times. Propagation of each rule meta-data is done only to the productions specified in the rule. For example: grammar_str = r''' MyRule {label: 'My Label', left}: 'first' {right, label: 'My overriden label'} | 'second'; MyRule {label: 'Other rule'}: 'third' {left} | 'fourth' {label: 'Fourth prod'}; ''' grammar = Grammar.from_string(grammar_str) my_rule = grammar.get_nonterminal('MyRule') # User meta-data is accessible on the non-terminal # Rule level meta-data are only those defined on the # first rule in the order of the definition. assert my_rule.label == 'My Label' prod1 = my_rule.productions[0] # First production overrides meta-data assert prod1.label == 'My overriden label' assert prod1.assoc == ASSOC_RIGHT # If not overriden it uses meta-data from the rule. prod2 = my_rule.productions[1] assert prod2.label == 'My Label' assert prod2.assoc == ASSOC_LEFT # Third and fourth production belongs to the second rule so they # inherits its meta-data. prod3 = my_rule.productions[2] assert prod3.label == 'Other rule' assert prod3.assoc == ASSOC_LEFT prod4 = my_rule.productions[3] assert prod4.label == 'Fourth prod' assert prod4.assoc == ASSOC_NONE","title":"User meta-data"},{"location":"grammar_language/#grammar-comments","text":"In parglare grammar, comments are available as both line comments and block comments: // This is a line comment. Everything from the '//' to the end of line is a comment. /* This is a block comment. Everything in between `/*` and '*/' is a comment. */","title":"Grammar comments"},{"location":"grammar_language/#handling-whitespaces-and-comments-in-your-language","text":"By default parser will skip whitespaces. Whitespace skipping is controlled by ws parameter to the parser which is by default set to '\\n\\t ' . If you need more control of the layout, i.e. handling of not only whitespaces but comments also, you can use a special rule LAYOUT : LAYOUT: LayoutItem | LAYOUT LayoutItem | EMPTY; LayoutItem: WS | Comment; terminals WS: /\\s+/; Comment: /\\/\\/.*/; This will form a separate layout parser that will parse in-between each matched tokens. In this example whitespaces and line-comments will be consumed by the layout parser. If this special rule is found in the grammar ws parser parameter is ignored. Here is another example that gives support for both line comments and block comments like the one used in the grammar language itself: LAYOUT: LayoutItem | LAYOUT LayoutItem | EMPTY; LayoutItem: WS | Comment; Comment: '/*' CorNCs '*/' | LineComment; CorNCs: CorNC | CorNCs CorNC | EMPTY; CorNC: Comment | NotComment | WS; terminals WS: /\\s+/; LineComment: /\\/\\/.*/; NotComment: /((\\*[^\\/])|[^\\s*\\/]|\\/[^\\*])+/; Tip If LAYOUT is provided it must match before the first token, between any two tokens in the input, and after the last token. If layout cannot be empty, the input cannot start or end with a token. If this is not desired, make sure to include EMPTY in the layout as one of its alternatives like in the previous examples.","title":"Handling whitespaces and comments in your language"},{"location":"grammar_language/#handling-keywords-in-your-language","text":"By default parser will match given string recognizer even if it is part of some larger word, i.e. it will not require matching on the word boundary. This is not the desired behavior for language keywords. For example, lets examine this little grammar: S: \"for\" name=ID \"=\" from=INT \"to\" to=INT; terminals ID: /\\w+/; INT: /\\d+/; This grammar is intended to match statement like this one: for a=10 to 20 But it will also match: fora=10 to20 which is not what we wanted. parglare allows the definition of a special terminal rule KEYWORD . This rule must define a regular expression recognizer . Any string recognizer in the grammar that can be also recognized by the KEYWORD recognizer is treated as a keyword and is changed during grammar construction to match only on word boundary. For example: S: \"for\" name=ID \"=\" from=INT \"to\" to=INT; terminals ID: /\\w+/; INT: /\\d+/; KEYWORD: /\\w+/; Now, fora=10 to20 will not be recognized as the words for and to are recognized to be keywords (they can be matched by the KEYWORD rule). This will be parsed correctly: for a=10 to 20 As = is not matched by the KEYWORD rule and thus doesn't require to be separated from the surrounding tokens. Note parglare uses integrated scanner so this example: for for=10 to 20 will be correctly parsed. for in for=10 will be recognized as ID and not as a keyword for , i.e. there is no lexical ambiguity due to tokenizer separation.","title":"Handling keywords in your language"},{"location":"grammar_modularization/","text":"Grammar modularization \u00b6 Grammar can be split across multiple files and imported using the import statement. This statement accepts a path to the target grammar file relative to the source grammar file and optional name of the target grammar after the as keyword. For example, import '../../othergrammar.pg'; or import '../../othergrammar.pg' as og; Rules from imported grammar can be referenced by fully qualified name consisting of dot-separated target grammar name and the rule name. By default the name of the target grammar is the base name of the grammar file. In the first example we can reference rules by using othergrammar. prefix as for example: SomeRule: INT othergrammar.SomeTargetRule+; Or in the second example we are using name og as the target grammar name so the previous example will be written as: SomeRule: INT og.SomeTargetRule+; import statement supports diamond imports as well as recursive imports. Each rule in the overall grammar has a fully qualified name (FQN). This name is constructed by the dot-separated chain of target grammar names ending with the rule name, using the first chain of imports that lead to the rule. This naming scheme enables the import of grammar files from arbitrary locations while still preserving a deterministic FQN for each rule of the grammar. For example, if there is a grammar file A.pg importing B.pg and C.pg , where B.pg also imports C.pg than, if the A.pg is the root of the grammar, all the rules in C.pg have FQN in the form of B.C.some_rule . Notice that the first path to C.pg was from B.pg as the B.pg grammar is imported first in A.pg. . See fqn tests for an example. Grammar file recognizers \u00b6 Grammar file can optionally provide its recognizers . These should be given in a Python file named <base grammar name>_recognizers.py and should be found in the same folder where the grammar file is found. For example, if grammar file is named mygrammar.pg than recognizers module should be named mygrammar_recognizers.py . For a parglare to be able to collect all recognizers defined in a module a collector is used. It is a decorator constructed in the Python recognizer module and used to decorate each recognizer function. For example, mygrammar_recognizers.py might be given as: from parglare import get_collector recognizer = get_collector() @recognizer def term_a(input, position): ... some recognition @recognizer def term_b(input, position): ... recognizer object is collector in this case. It will construct a dictionary of all recognizers decorated by it and that dictionary will be provided as recognizer.all . parglare recognizer loader will implicitly search for recognizer.all . By default, a name of a decorated function will serve as a terminal name this recognizer is defined for. But, you can provide different name using a string parameter to recognizer decorator, like: @recognizer('NUMERIC_ID') def number(input, pos): ... In this case grammar terminal is named NUMERIC_ID while the recognition function is named number . This can be used, for example, to create a library of common recognizer function and use them in grammar for terminals with different names, like: from somemodule import myrecognizer from parglare import get_collector recognizer = get_collector() recognizer('NUMERIC_ID')(myrecognizer) You can use a fully qualified terminal name to override recognizer for imported terminal: @recognizer('base.COMMA') def comma_recognizer(input, pos): if input[pos] == ',': return input[pos:pos + 1] In this case there is an imported grammar base whose terminal COMMA recognizer has been overridden by comma_recognizer recognizer function. Warning Since the way a recognizer module is imported in Python you must use only Python absolute module imports inside the recognizer module. Recognizer search order \u00b6 Recognizers are loaded from a grammar module but can be overridden from importing grammars, using FQN of the terminal, or by using recognizers parameter of the grammar. Grammar file actions \u00b6 Similarly to recognizers, actions can be provided in a Python file named <base grammar name>_actions.py that should be found in the same folder where the grammar file is found. For example, if grammar file is named mygrammar.pg than actions module should be named mygrammar_actions.py . For a parglare to be able to collect all actions defined in a module a collector is used in very much the same way as it is used for recognizers. It is a decorator constructed in the Python actions module and used to decorate each action function. For example, mygrammar_actions.py might be given as: from parglare import get_collector action = get_collector() @action def first_rule(context, nodes): ... @action def second_rule(context, nodes): ... action object is collector in this case. It will construct a dictionary of all actions decorated by it and that dictionary will be provided as action.all . parglare action loader will implicitly search for action.all . By default, a name of a decorated function will serve as a grammar symbol name or in-grammar defined action name (using @ , see syntax for action specification ) this action is defined for. But, you can provide different name using a string parameter to action decorator: Warning Since the way an action module is imported in Python you must use only Python absolute module imports inside of it. Action search order \u00b6 Actions are searched in the order of specificity by searching the following (in the given order): Actions given using actions parameter by FQN of the symbol, Actions loaded from actions module using FQN of the symbol, Actions given using actions parameter by FQN of the action, Actions loaded from actions module using FQN of the action, Actions given using actions parameter by symbol name, Actions loaded from actions module using symbol name, Actions given using actions parameter by action name, Actions loaded from actions module using action name, parglare built-in actions using action name.","title":"Modularization"},{"location":"grammar_modularization/#grammar-modularization","text":"Grammar can be split across multiple files and imported using the import statement. This statement accepts a path to the target grammar file relative to the source grammar file and optional name of the target grammar after the as keyword. For example, import '../../othergrammar.pg'; or import '../../othergrammar.pg' as og; Rules from imported grammar can be referenced by fully qualified name consisting of dot-separated target grammar name and the rule name. By default the name of the target grammar is the base name of the grammar file. In the first example we can reference rules by using othergrammar. prefix as for example: SomeRule: INT othergrammar.SomeTargetRule+; Or in the second example we are using name og as the target grammar name so the previous example will be written as: SomeRule: INT og.SomeTargetRule+; import statement supports diamond imports as well as recursive imports. Each rule in the overall grammar has a fully qualified name (FQN). This name is constructed by the dot-separated chain of target grammar names ending with the rule name, using the first chain of imports that lead to the rule. This naming scheme enables the import of grammar files from arbitrary locations while still preserving a deterministic FQN for each rule of the grammar. For example, if there is a grammar file A.pg importing B.pg and C.pg , where B.pg also imports C.pg than, if the A.pg is the root of the grammar, all the rules in C.pg have FQN in the form of B.C.some_rule . Notice that the first path to C.pg was from B.pg as the B.pg grammar is imported first in A.pg. . See fqn tests for an example.","title":"Grammar modularization"},{"location":"grammar_modularization/#grammar-file-recognizers","text":"Grammar file can optionally provide its recognizers . These should be given in a Python file named <base grammar name>_recognizers.py and should be found in the same folder where the grammar file is found. For example, if grammar file is named mygrammar.pg than recognizers module should be named mygrammar_recognizers.py . For a parglare to be able to collect all recognizers defined in a module a collector is used. It is a decorator constructed in the Python recognizer module and used to decorate each recognizer function. For example, mygrammar_recognizers.py might be given as: from parglare import get_collector recognizer = get_collector() @recognizer def term_a(input, position): ... some recognition @recognizer def term_b(input, position): ... recognizer object is collector in this case. It will construct a dictionary of all recognizers decorated by it and that dictionary will be provided as recognizer.all . parglare recognizer loader will implicitly search for recognizer.all . By default, a name of a decorated function will serve as a terminal name this recognizer is defined for. But, you can provide different name using a string parameter to recognizer decorator, like: @recognizer('NUMERIC_ID') def number(input, pos): ... In this case grammar terminal is named NUMERIC_ID while the recognition function is named number . This can be used, for example, to create a library of common recognizer function and use them in grammar for terminals with different names, like: from somemodule import myrecognizer from parglare import get_collector recognizer = get_collector() recognizer('NUMERIC_ID')(myrecognizer) You can use a fully qualified terminal name to override recognizer for imported terminal: @recognizer('base.COMMA') def comma_recognizer(input, pos): if input[pos] == ',': return input[pos:pos + 1] In this case there is an imported grammar base whose terminal COMMA recognizer has been overridden by comma_recognizer recognizer function. Warning Since the way a recognizer module is imported in Python you must use only Python absolute module imports inside the recognizer module.","title":"Grammar file recognizers"},{"location":"grammar_modularization/#recognizer-search-order","text":"Recognizers are loaded from a grammar module but can be overridden from importing grammars, using FQN of the terminal, or by using recognizers parameter of the grammar.","title":"Recognizer search order"},{"location":"grammar_modularization/#grammar-file-actions","text":"Similarly to recognizers, actions can be provided in a Python file named <base grammar name>_actions.py that should be found in the same folder where the grammar file is found. For example, if grammar file is named mygrammar.pg than actions module should be named mygrammar_actions.py . For a parglare to be able to collect all actions defined in a module a collector is used in very much the same way as it is used for recognizers. It is a decorator constructed in the Python actions module and used to decorate each action function. For example, mygrammar_actions.py might be given as: from parglare import get_collector action = get_collector() @action def first_rule(context, nodes): ... @action def second_rule(context, nodes): ... action object is collector in this case. It will construct a dictionary of all actions decorated by it and that dictionary will be provided as action.all . parglare action loader will implicitly search for action.all . By default, a name of a decorated function will serve as a grammar symbol name or in-grammar defined action name (using @ , see syntax for action specification ) this action is defined for. But, you can provide different name using a string parameter to action decorator: Warning Since the way an action module is imported in Python you must use only Python absolute module imports inside of it.","title":"Grammar file actions"},{"location":"grammar_modularization/#action-search-order","text":"Actions are searched in the order of specificity by searching the following (in the given order): Actions given using actions parameter by FQN of the symbol, Actions loaded from actions module using FQN of the symbol, Actions given using actions parameter by FQN of the action, Actions loaded from actions module using FQN of the action, Actions given using actions parameter by symbol name, Actions loaded from actions module using symbol name, Actions given using actions parameter by action name, Actions loaded from actions module using action name, parglare built-in actions using action name.","title":"Action search order"},{"location":"handling_errors/","text":"Handling errors \u00b6 When parglare encounters a situation in which no SHIFT or REDUCE operation could be performed it will report an error by raising an instance of parglare.ParseError class. ParseError has the following attributes: location - an instance of the Location class with information of the span of the error. symbols_expected (list) - a list of expected symbol at the location. tokens_ahead (list) - a list of tokens recognized at the position by trying all terminal symbols recognizers from the grammar. Note that this list might be empty in case nothing can be recognized at the position or it might have more than one element if more recognizers succeeds (lexical ambiguity). symbols_before (list) - a list of last seen symbols. In the case of LR parser it will always be a single element list. In the case of GLR there might be more symbols if there were multiple parser heads. last_heads (list) - A list of last GLR parser heads. Available only for GLR parsing. grammar (Grammar) - An instance of parglare.Grammar class used for parsing. Error recovery \u00b6 There are a lot of situations where you would want parser to report all the errors in one go. To do this, parser has to recover from errors, i.e. get to the valid state and continue. To enable error recovery set error_recovery parameter of parser construction to True . This will enable implicit error recovery strategy that will try to search for expected tokens in the input ahead and when the first is found the parsing will continue. All errors will be collected as an errors list on the parser instance. Each error is an instance of ParseError class . In case no recovery is possible last ParseError will be raised. ParserError has a location which represents the span of the error in the input (e.g. error.location.start_position and error.location.end_position ). Custom recovery strategy \u00b6 To provide a custom strategy for error recovery set error_recovery parser constructor parameter to a Python function. This function should have the following signature: def error_recovery_strategy(context, error): ... context *- context like object (usually the parser head). error - ParseError instance . Using the head object you can query the state of the parser. E.g. to get the position use context.position , to get the parser state use context.state , to get expected symbols in this state use context.state.actions.keys() . To get information about the error use error object. E.g. to get expected symbols at this position for which parser can successfully continue use error.symbols_expected . The recovery function should modify the head (e.g. its position and/or token_ahead ) and bring it to a state which can continue. If the recovery is successful the function should return True , otherwise False . You can call a default error recovery from your custom recovery by context.parser.default_error_recovery(context)","title":"Handling errors"},{"location":"handling_errors/#handling-errors","text":"When parglare encounters a situation in which no SHIFT or REDUCE operation could be performed it will report an error by raising an instance of parglare.ParseError class. ParseError has the following attributes: location - an instance of the Location class with information of the span of the error. symbols_expected (list) - a list of expected symbol at the location. tokens_ahead (list) - a list of tokens recognized at the position by trying all terminal symbols recognizers from the grammar. Note that this list might be empty in case nothing can be recognized at the position or it might have more than one element if more recognizers succeeds (lexical ambiguity). symbols_before (list) - a list of last seen symbols. In the case of LR parser it will always be a single element list. In the case of GLR there might be more symbols if there were multiple parser heads. last_heads (list) - A list of last GLR parser heads. Available only for GLR parsing. grammar (Grammar) - An instance of parglare.Grammar class used for parsing.","title":"Handling errors"},{"location":"handling_errors/#error-recovery","text":"There are a lot of situations where you would want parser to report all the errors in one go. To do this, parser has to recover from errors, i.e. get to the valid state and continue. To enable error recovery set error_recovery parameter of parser construction to True . This will enable implicit error recovery strategy that will try to search for expected tokens in the input ahead and when the first is found the parsing will continue. All errors will be collected as an errors list on the parser instance. Each error is an instance of ParseError class . In case no recovery is possible last ParseError will be raised. ParserError has a location which represents the span of the error in the input (e.g. error.location.start_position and error.location.end_position ).","title":"Error recovery"},{"location":"handling_errors/#custom-recovery-strategy","text":"To provide a custom strategy for error recovery set error_recovery parser constructor parameter to a Python function. This function should have the following signature: def error_recovery_strategy(context, error): ... context *- context like object (usually the parser head). error - ParseError instance . Using the head object you can query the state of the parser. E.g. to get the position use context.position , to get the parser state use context.state , to get expected symbols in this state use context.state.actions.keys() . To get information about the error use error object. E.g. to get expected symbols at this position for which parser can successfully continue use error.symbols_expected . The recovery function should modify the head (e.g. its position and/or token_ahead ) and bring it to a state which can continue. If the recovery is successful the function should return True , otherwise False . You can call a default error recovery from your custom recovery by context.parser.default_error_recovery(context)","title":"Custom recovery strategy"},{"location":"lr_parsing/","text":"LR parsing, ambiguities and conflicts resolving \u00b6 LR parser operates as a deterministic PDA (Push-down automata). It is a state machine which is always in some state during parsing. The state machine must deterministically decide what is the next state just based on its current state and one token of lookahead (actually, parser could use more than one token of lookahead thus being LR(2), LR(3) and so on, but they are not very practical). This decision is given by LR tables which are precalculated from the grammar before parsing even begins. For example, let's see what happens if we have a simple expression grammar: E: E '+' E | E '*' E | number; number: /\\d+/; and we want to parse the following input: 1 + 2 * 3 Language defined by this grammar is ambiguous as the expression can be interpreted either as: ((1 + 2) * 3) or: (1 + (2 * 3)) In the parsing process, parser starts in state 0 and it sees token 1 ahead (one lookahead is used - LR(1)). The only thing a parser can do in this case is to shift, i.e. to consume the token and advance the position. This operation transition the automata to some other state. From each state there is only one valid transition that can be taken or the PDA won't be deterministic, i.e. we could simultaneously follow different paths (that is exactly what the GLRParser does). Current position would be (the dot represents the position): 1 . + 2 * 3 Now the parser sees + token ahead and the tables will tell him to reduce the number he just saw to E (a number is an expression according to the grammar). Thus, on the stack the parser will have an expression E (actually, LR states are kept on the stack but that's not important for this little analysis). This reduction will advance PDA to some other state again. Each shift/reduce operation change state so I'll not repeat that anymore. Tip See pglr command which can be used to visualize PDA. Try to visualize automata for this grammar. After reduction parser will do shift of + token. There is nothing to reduce as the sub-expression on stack is E + which can't be reduced as it's not complete. So, the only thing we can do is to shift 2 token. Now, the position is: 1 + 2 . * 3 And the stack is: E + 2 And this is a place where the parser can't decide what to do. It can either reduce the sum on the stack or shift * and 3 and reduce multiplication first and the sumation afterwards. If the sum is reduced first and * shifted afterwards we would get the following result: (1 + 2) * 3 If the shift of * and 3 is done instead of reducing, the reduction would first reduce multiplication and then sum (reduction is always done on the top of the stack). We will have the following result: 1 + (2 * 3) From the point of view of arithmetic priorities, preferred solution is the last one but the parser don't know arithmetic rules. If you analyze this grammar using pglr command you will see that the LR tables have Shift/Reduce conflicts as there is a state in which parser can't decide whether to shift or to reduce (we just saw that situation). parglare gives you various tools to be more explicit with your grammar and to resolve these conflicts. There are two situations when conflicts can't be resolved: you need more than one lookahead to disambiguate, your language is inherently ambiguous. If you end up in one of these situations you must use GLR parsing, which will fork the parser at each state which has multiple paths, and explore all possibilities. Resolving conflicts \u00b6 When we run: $ pglr compile expr.pg where in expr.pg we have the above grammar, we get the following output at the end: *** S/R conflicts *** There are 4 S/R conflicts State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '+' can't decide whether to shift or reduce by production(s) '1: E = E + E'. State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '*' can't decide whether to shift or reduce by production(s) '1: E = E + E'. State 7 2: E = E * E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 7 and input symbol '+' can't decide whether to shift or reduce by production(s) '2: E = E * E'. State 7 2: E = E * E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 7 and input symbol '*' can't decide whether to shift or reduce by production(s) '2: E = E * E'. Grammar OK. There are 4 Shift/Reduce conflicts. Either use 'prefer_shifts' parser mode, try to resolve manually or use GLR parsing. As we can see this grammar has 4 Shift/Reduce conflicts. At the end of the output we get an advice to either use prefer_shifts strategy that will always prefer shift over reduce. In this case that's not what we want. If we look closely at the output we see that parglare gives us an informative explanation why there are conflicts in our grammar. The first conflict: State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '+' can't decide whether to shift or reduce by production(s) '1: E = E + E'. Tell us that when the parser saw addition \u2014 the dot in the above productions represents all possible positions of the parser in the input stream in this state \u2014 and there is + ahead, it doesn't know should it reduce the addition or shift the + token. This means that if we have an expression: 1 + 2 + 3 should we calculate it as (1 + 2) + 3 or as 1 + (2 + 3) . Of course, the result in this case would be the same, but imagine what would happen if we had subtraction operation instead of addition. In arithmetic, this is defined by association which says that addition if left associative, thus the operation is executed from left to right. Parglare enables you to define associativity for you productions by specifying {left} or {right} at the end of production. You can think of left associativity as telling the parser to prefer reduce over shift for this production and the right associativity for preferring shifts over reduces. Let's see the second conflict: State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '*' can't decide whether to shift or reduce by production(s) '1: E = E + E'. In the same state, when we saw addition and have * ahead parser can't decide. This means that if we have an expression: 1 + 2 * 3 should we calculate it as (1 + 2) * 3 or 1 + (2 * 3) . In arithmetic this is handled by operation priority. We want multiplication to be executed first, so we should raise the priority of multiplication (or lower the priority of addition). E: E '+' E {left, 1} | E '*' E {left, 2} | number; number: /\\d+/; We have augmented our grammar to state that both operation are left associative, thus the parser will know what to do in the case of 1 + 2 + 3 or 1 * 2 * 3 it will reduce from left to right, i.e. prefer reduce over shifts. We also specified that addition has a priority of 1 and multiplication has a priority of 2, thus the parser will know what to do in case of 1 + 2 * 3 , it will shift * instead of reducing addition as the multiplication should be reduced/calculated first. Note The default priority for rules is 10. This change in the grammar resolves all ambiguities and our grammar is now LR(1). See the section on disambiguation strategies for more.","title":"LR parsing and conflicts"},{"location":"lr_parsing/#lr-parsing-ambiguities-and-conflicts-resolving","text":"LR parser operates as a deterministic PDA (Push-down automata). It is a state machine which is always in some state during parsing. The state machine must deterministically decide what is the next state just based on its current state and one token of lookahead (actually, parser could use more than one token of lookahead thus being LR(2), LR(3) and so on, but they are not very practical). This decision is given by LR tables which are precalculated from the grammar before parsing even begins. For example, let's see what happens if we have a simple expression grammar: E: E '+' E | E '*' E | number; number: /\\d+/; and we want to parse the following input: 1 + 2 * 3 Language defined by this grammar is ambiguous as the expression can be interpreted either as: ((1 + 2) * 3) or: (1 + (2 * 3)) In the parsing process, parser starts in state 0 and it sees token 1 ahead (one lookahead is used - LR(1)). The only thing a parser can do in this case is to shift, i.e. to consume the token and advance the position. This operation transition the automata to some other state. From each state there is only one valid transition that can be taken or the PDA won't be deterministic, i.e. we could simultaneously follow different paths (that is exactly what the GLRParser does). Current position would be (the dot represents the position): 1 . + 2 * 3 Now the parser sees + token ahead and the tables will tell him to reduce the number he just saw to E (a number is an expression according to the grammar). Thus, on the stack the parser will have an expression E (actually, LR states are kept on the stack but that's not important for this little analysis). This reduction will advance PDA to some other state again. Each shift/reduce operation change state so I'll not repeat that anymore. Tip See pglr command which can be used to visualize PDA. Try to visualize automata for this grammar. After reduction parser will do shift of + token. There is nothing to reduce as the sub-expression on stack is E + which can't be reduced as it's not complete. So, the only thing we can do is to shift 2 token. Now, the position is: 1 + 2 . * 3 And the stack is: E + 2 And this is a place where the parser can't decide what to do. It can either reduce the sum on the stack or shift * and 3 and reduce multiplication first and the sumation afterwards. If the sum is reduced first and * shifted afterwards we would get the following result: (1 + 2) * 3 If the shift of * and 3 is done instead of reducing, the reduction would first reduce multiplication and then sum (reduction is always done on the top of the stack). We will have the following result: 1 + (2 * 3) From the point of view of arithmetic priorities, preferred solution is the last one but the parser don't know arithmetic rules. If you analyze this grammar using pglr command you will see that the LR tables have Shift/Reduce conflicts as there is a state in which parser can't decide whether to shift or to reduce (we just saw that situation). parglare gives you various tools to be more explicit with your grammar and to resolve these conflicts. There are two situations when conflicts can't be resolved: you need more than one lookahead to disambiguate, your language is inherently ambiguous. If you end up in one of these situations you must use GLR parsing, which will fork the parser at each state which has multiple paths, and explore all possibilities.","title":"LR parsing, ambiguities and conflicts resolving"},{"location":"lr_parsing/#resolving-conflicts","text":"When we run: $ pglr compile expr.pg where in expr.pg we have the above grammar, we get the following output at the end: *** S/R conflicts *** There are 4 S/R conflicts State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '+' can't decide whether to shift or reduce by production(s) '1: E = E + E'. State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '*' can't decide whether to shift or reduce by production(s) '1: E = E + E'. State 7 2: E = E * E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 7 and input symbol '+' can't decide whether to shift or reduce by production(s) '2: E = E * E'. State 7 2: E = E * E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 7 and input symbol '*' can't decide whether to shift or reduce by production(s) '2: E = E * E'. Grammar OK. There are 4 Shift/Reduce conflicts. Either use 'prefer_shifts' parser mode, try to resolve manually or use GLR parsing. As we can see this grammar has 4 Shift/Reduce conflicts. At the end of the output we get an advice to either use prefer_shifts strategy that will always prefer shift over reduce. In this case that's not what we want. If we look closely at the output we see that parglare gives us an informative explanation why there are conflicts in our grammar. The first conflict: State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '+' can't decide whether to shift or reduce by production(s) '1: E = E + E'. Tell us that when the parser saw addition \u2014 the dot in the above productions represents all possible positions of the parser in the input stream in this state \u2014 and there is + ahead, it doesn't know should it reduce the addition or shift the + token. This means that if we have an expression: 1 + 2 + 3 should we calculate it as (1 + 2) + 3 or as 1 + (2 + 3) . Of course, the result in this case would be the same, but imagine what would happen if we had subtraction operation instead of addition. In arithmetic, this is defined by association which says that addition if left associative, thus the operation is executed from left to right. Parglare enables you to define associativity for you productions by specifying {left} or {right} at the end of production. You can think of left associativity as telling the parser to prefer reduce over shift for this production and the right associativity for preferring shifts over reduces. Let's see the second conflict: State 6 1: E = E + E . {+, *, STOP} 1: E = E . + E {+, *, STOP} 2: E = E . * E {+, *, STOP} In state 6 and input symbol '*' can't decide whether to shift or reduce by production(s) '1: E = E + E'. In the same state, when we saw addition and have * ahead parser can't decide. This means that if we have an expression: 1 + 2 * 3 should we calculate it as (1 + 2) * 3 or 1 + (2 * 3) . In arithmetic this is handled by operation priority. We want multiplication to be executed first, so we should raise the priority of multiplication (or lower the priority of addition). E: E '+' E {left, 1} | E '*' E {left, 2} | number; number: /\\d+/; We have augmented our grammar to state that both operation are left associative, thus the parser will know what to do in the case of 1 + 2 + 3 or 1 * 2 * 3 it will reduce from left to right, i.e. prefer reduce over shifts. We also specified that addition has a priority of 1 and multiplication has a priority of 2, thus the parser will know what to do in case of 1 + 2 * 3 , it will shift * instead of reducing addition as the multiplication should be reduced/calculated first. Note The default priority for rules is 10. This change in the grammar resolves all ambiguities and our grammar is now LR(1). See the section on disambiguation strategies for more.","title":"Resolving conflicts"},{"location":"parse_forest_trees/","text":"GLR forest \u00b6 The GLR parser returns the parse forest ( Forest object). The forest is created in an efficient way to store all the possible solutions without exponential explosion. This structure is called Shared Packed Parse Forest (SPPF). The forest is iterable and indexable structure. For example: forest = GLRParser(grammar).parse(some_input) for tree in forest: ... do something tree = forest[3] # To get the number of trees/solutions print(len(forest)) # or print(forest.solutions) # To get the total number of ambiguities in the forest. print(forest.ambiguities) Forest trees are lazily constructed during iteration and accessing of tree nodes. Both forest and parse trees have to_str and to_dot methods to render as string/dot. Tip You can use to_str() on the forest get the string representation with all ambiguities indicated. This can be used to analyze ambiguity. For example: parser = GLRParser(g) forest = parser.parse_file('some_file') with open('forest.txt', 'w') as f: f.write(forest.to_str()) This can also be done with pglr parse command. Another option is to use to_str() on individual trees and then use diffing tool to compare. For example: parser = GLRParser(g) forest = parser.parse_file('some_file') for idx, tree in enumerate(forest): with open(f'tree_{idx:03d}.txt', 'w') as f: f.write(tree.to_str()) Now you can run any diff tool on the produced outputs to see where are the ambiguities: $ meld tree_000.txt tree_001.txt For smaller inputs you can also use to_dot to display a forest/tree as a graph. There is also forest.get_tree(idx) which is the same as forest[idx] , i.e. it returns lazy tree. To get non-lazy tree, i.e. tree whose proxies are pre-initialized, call forest.get_nonlazy_tree(idx) . If you just need the first tree, call forest.get_first_tree() to get an unpacked tree which consists only of NodeTerm and NodeNonTerm . This tree is the fastest to navigate but only tree 0 is supported in this form. Parse trees \u00b6 Parse trees are produced by GLR parse forest. Parse trees are also returned by the LR parser if build_tree parser constructor parameter is set to True . The nodes of parse trees are instances of either NodeTerm for terminal nodes (leafs of the tree) or NodeNonTerm for non-terminal nodes (intermediate nodes). Each node of the tree has following attributes: start_position/end_position - the start and end position in the input stream where the node starts/ends. It is given in absolute 0-based offset. To convert to line/column format for textual inputs you can use parglare.pos_to_line_col(input_str, position) function which returns tuple (line, column) . Of course, this call doesn't make any sense if you are parsing a non-textual content. layout_content - the layout that precedes the given tree node. The layout consists of whitespaces/comments. symbol (property) - a grammar symbol this node is created for. Additionally, each NodeTerm has: value - the value (a part of input_str) which this terminal represents. It is equivalent to input_str[start_position:end_position] . additional_data - a list of additional information returned by a custom recognizer. This gets passed to terminal nodes actions if call_actions is called for the parse tree. Additionally, each NodeNonTerm has: children - sub-nodes which are also of NodeNonTerm / NodeTerm type. NodeNonTerm is iterable. Iterating over it will iterate over its children. production - a grammar production whose reduction created this node. Each node has a to_str() method which will return a string representation of the sub-tree starting from the given node. If called on a root node it will return the string representation of the whole tree. For example, parsing the input 1 + 2 * 3 - 1 with the expression grammar from the quick start will look like this if printed with to_str() : E[0] E[0] E[0] number[0, 1] +[2, +] E[4] E[4] number[4, 2] *[6, *] E[8] number[8, 3] -[10, -] E[11] number[11, 1] Visitor \u00b6 A visitor function is provided for depth-first processing of a tree-like structures (actually can be applied to graphs also). The signature of visitor is: def visitor(root, iterator, visit, memoize=True, check_cycle=False): Where: root is the root element of the structure to process iterator iterator is a callable that should return an iterator for the given element yielding the next elements to process. E.g. for a tree node iterator callable should return an iterator yielding children nodes. visit is a function called when the node is visited. It results will be passed into visitors higher in the hierarchy (thus enabling bottom-up processing). visit function should accept three parameters: current tree node, sub-results from lower-level visitors and the depth of the current tree node. memoize - Should results be cached. Handy for direct acyclic graphs if we want to prevent multiple calculation of the same sub-graph. check_cycle - If set to True will prevent traversing of cyclic structure by keeping cache of already visited nodes and throwing LoopError if cycle is detected. visitor is used internally by parglare so the best place to see how it is used is the parglare code itself.","title":"Parse forest/trees"},{"location":"parse_forest_trees/#glr-forest","text":"The GLR parser returns the parse forest ( Forest object). The forest is created in an efficient way to store all the possible solutions without exponential explosion. This structure is called Shared Packed Parse Forest (SPPF). The forest is iterable and indexable structure. For example: forest = GLRParser(grammar).parse(some_input) for tree in forest: ... do something tree = forest[3] # To get the number of trees/solutions print(len(forest)) # or print(forest.solutions) # To get the total number of ambiguities in the forest. print(forest.ambiguities) Forest trees are lazily constructed during iteration and accessing of tree nodes. Both forest and parse trees have to_str and to_dot methods to render as string/dot. Tip You can use to_str() on the forest get the string representation with all ambiguities indicated. This can be used to analyze ambiguity. For example: parser = GLRParser(g) forest = parser.parse_file('some_file') with open('forest.txt', 'w') as f: f.write(forest.to_str()) This can also be done with pglr parse command. Another option is to use to_str() on individual trees and then use diffing tool to compare. For example: parser = GLRParser(g) forest = parser.parse_file('some_file') for idx, tree in enumerate(forest): with open(f'tree_{idx:03d}.txt', 'w') as f: f.write(tree.to_str()) Now you can run any diff tool on the produced outputs to see where are the ambiguities: $ meld tree_000.txt tree_001.txt For smaller inputs you can also use to_dot to display a forest/tree as a graph. There is also forest.get_tree(idx) which is the same as forest[idx] , i.e. it returns lazy tree. To get non-lazy tree, i.e. tree whose proxies are pre-initialized, call forest.get_nonlazy_tree(idx) . If you just need the first tree, call forest.get_first_tree() to get an unpacked tree which consists only of NodeTerm and NodeNonTerm . This tree is the fastest to navigate but only tree 0 is supported in this form.","title":"GLR forest"},{"location":"parse_forest_trees/#parse-trees","text":"Parse trees are produced by GLR parse forest. Parse trees are also returned by the LR parser if build_tree parser constructor parameter is set to True . The nodes of parse trees are instances of either NodeTerm for terminal nodes (leafs of the tree) or NodeNonTerm for non-terminal nodes (intermediate nodes). Each node of the tree has following attributes: start_position/end_position - the start and end position in the input stream where the node starts/ends. It is given in absolute 0-based offset. To convert to line/column format for textual inputs you can use parglare.pos_to_line_col(input_str, position) function which returns tuple (line, column) . Of course, this call doesn't make any sense if you are parsing a non-textual content. layout_content - the layout that precedes the given tree node. The layout consists of whitespaces/comments. symbol (property) - a grammar symbol this node is created for. Additionally, each NodeTerm has: value - the value (a part of input_str) which this terminal represents. It is equivalent to input_str[start_position:end_position] . additional_data - a list of additional information returned by a custom recognizer. This gets passed to terminal nodes actions if call_actions is called for the parse tree. Additionally, each NodeNonTerm has: children - sub-nodes which are also of NodeNonTerm / NodeTerm type. NodeNonTerm is iterable. Iterating over it will iterate over its children. production - a grammar production whose reduction created this node. Each node has a to_str() method which will return a string representation of the sub-tree starting from the given node. If called on a root node it will return the string representation of the whole tree. For example, parsing the input 1 + 2 * 3 - 1 with the expression grammar from the quick start will look like this if printed with to_str() : E[0] E[0] E[0] number[0, 1] +[2, +] E[4] E[4] number[4, 2] *[6, *] E[8] number[8, 3] -[10, -] E[11] number[11, 1]","title":"Parse trees"},{"location":"parse_forest_trees/#visitor","text":"A visitor function is provided for depth-first processing of a tree-like structures (actually can be applied to graphs also). The signature of visitor is: def visitor(root, iterator, visit, memoize=True, check_cycle=False): Where: root is the root element of the structure to process iterator iterator is a callable that should return an iterator for the given element yielding the next elements to process. E.g. for a tree node iterator callable should return an iterator yielding children nodes. visit is a function called when the node is visited. It results will be passed into visitors higher in the hierarchy (thus enabling bottom-up processing). visit function should accept three parameters: current tree node, sub-results from lower-level visitors and the depth of the current tree node. memoize - Should results be cached. Handy for direct acyclic graphs if we want to prevent multiple calculation of the same sub-graph. check_cycle - If set to True will prevent traversing of cyclic structure by keeping cache of already visited nodes and throwing LoopError if cycle is detected. visitor is used internally by parglare so the best place to see how it is used is the parglare code itself.","title":"Visitor"},{"location":"parser/","text":"Parser parameters \u00b6 There are several parameters you can pass during the parser construction. The mandatory first parameter is the Grammar instance. Other parameters are explained in the rest of this section. All parameters described here work for both parglare.Parser and parglare.GLRParser classes. actions \u00b6 This parameters is a dict of actions keyed by the name of the grammar rule. layout_actions \u00b6 This parameter is used to specify actions called when the rules of layout sub-grammar are reduced. This is rarely needed but there are times when you would like to process matched layout (e.g. whitespaces, comments). It is given in the same format as actions parameter, a dict of callables keyed by grammar rule names. ws \u00b6 This parameter specifies a string whose characters are considered to be whitespace. By default its value is '\\n\\r\\t ' . It is used if layout sub-grammar ( LAYOUT grammar rule) is not defined. If LAYOUT rule is given in the grammar it is used instead and this parameter is ignored. build_tree \u00b6 A boolean whose default value is False . If set to True parser will call implicit actions that will build the parse tree . call_actions_during_tree_build \u00b6 By default, this parameter is set to False . If set to True , parser will call actions during the parse tree parse tree building process. The return value of each action will be discarded, since they directly affect the parse tree building process. Warning Use this parameter with a special care when GLR is used, since actions will be called even on trees that can't be completed (unsuccessful parses). consume_input \u00b6 A boolean whose value is True by default. If True the whole input must be consumed for the parse to be considered successful. This is most of the time what you want. If set to False then LR parser will parse as much as possible and leave the rest of the input unconsumed while GLR parser will produce all possible parses with both completely and incompletely consumed input. Warning Be aware that setting this option to False for GLR usually leads to high level of ambiguity and multiple parses as any substring from beginning of the input that parses will be considered a valid parse. prefer_shifts \u00b6 By default set to True for LR parser and to False for GLR parser. In case of shift/reduce conflicts this strategy would favor shift over reduce. You can still use associativity rules to decide per production. You can disable this rule on per-production basis by using nops on the production. Warning Do not use prefer_shifts if you don't understand the implications. Try to understand conflicts and resolution strategies . prefer_shifts_over_empty \u00b6 By default set to True for LR parser and to False for GLR parser. In case of shift/reduce conflicts on empty reductions this strategy would favor shift over reduce. You can still use associativity rules to decide per production. You can disable this rule on per-production basis by using nopse on the production. Warning Do not use prefer_shifts_over_empty if you don't understand the implications. Try to understand conflicts and resolution strategies . error_recovery \u00b6 By default set to False . If set to True default error recovery will be used. If set to a Python function, the function will be called to recover from errors. For more information see Error recovery . debug/debug_layout \u00b6 This parameter if set to True will put the parser in debug mode. In this mode parser will print a detailed information of its actions to the standard output. To put layout subparser in the debug mode use the debug_layout parameter. Both parameters are set to False by default. For more information see Debugging debug_colors \u00b6 Set this to True to enable terminal colors in debug/trace output. False by default. tables \u00b6 The value of this parameter is either parglare.LALR or parglare.SLR and it is used to choose the type of LR tables to create. By default LALR tables are used with a slight twist to avoid Reduce/Reduce conflicts that may happen with pure LALR tables. This parameter should not be used in normal circumstances and is provided more for experimentation purposes. force_load_table \u00b6 LR table is loaded from <grammar_file_name>.pgt file if the file exists and is newer than all of the grammar files, root and imported. If any of the grammar file modification time is greater than the modification time of the cached LR table file, table is recalculated and persisted. If you are deploying the parser in a way that will change file modification times which would trigger table calculation you can set force_load_table to True . If this flag is set no modification check will be performed and table calculation will happen only if .pgt file doesn't exist. table \u00b6 You can pass precomputed parsing table here. This is useful for implementing custom parse table caching. None value for this parameter (the default) instructs parser to build (or fetch from cache) it's own tables internally. Example flow for custom caching is shown in an example . Warning Be careful to provide parse tables compatible with parser type. Passing tables containing conflicts to Parser class will probably result in an error, but passing tables with automatically resolved conflicts ( prefer_shifts=True ) to GLRParser will result in parser which may skip proper parses. parse and parse_file calls \u00b6 parse call is used to parse input string or list of objects. For parsing of textual file parse_file is used. These two calls accepts the following parameters: input_str - first positional and mandatory parameter only for parse call - the input string/list of objects. position - the start position to parse from. By default 0. extra - an object used for arbitrary user state kept during parsing. It will be accessible on context-like objects. If not given an instance of dict will be created. file_name - first positional and mandatory parameter only for parse_file call - the name/path of the file to parse. Token class \u00b6 This class from parglare.parser is used to represent lookahead tokens. Token is a concrete matched terminal from the input stream. Attributes: symbol ( Terminal ) - terminal grammar symbol represented by this token, value ( list or str ) - matched part of the input stream, additional_data ( list ) - additional information returned by a custom recognizer. length ( int ) - length of the matched input.","title":"Parser"},{"location":"parser/#parser-parameters","text":"There are several parameters you can pass during the parser construction. The mandatory first parameter is the Grammar instance. Other parameters are explained in the rest of this section. All parameters described here work for both parglare.Parser and parglare.GLRParser classes.","title":"Parser parameters"},{"location":"parser/#actions","text":"This parameters is a dict of actions keyed by the name of the grammar rule.","title":"actions"},{"location":"parser/#layout_actions","text":"This parameter is used to specify actions called when the rules of layout sub-grammar are reduced. This is rarely needed but there are times when you would like to process matched layout (e.g. whitespaces, comments). It is given in the same format as actions parameter, a dict of callables keyed by grammar rule names.","title":"layout_actions"},{"location":"parser/#ws","text":"This parameter specifies a string whose characters are considered to be whitespace. By default its value is '\\n\\r\\t ' . It is used if layout sub-grammar ( LAYOUT grammar rule) is not defined. If LAYOUT rule is given in the grammar it is used instead and this parameter is ignored.","title":"ws"},{"location":"parser/#build_tree","text":"A boolean whose default value is False . If set to True parser will call implicit actions that will build the parse tree .","title":"build_tree"},{"location":"parser/#call_actions_during_tree_build","text":"By default, this parameter is set to False . If set to True , parser will call actions during the parse tree parse tree building process. The return value of each action will be discarded, since they directly affect the parse tree building process. Warning Use this parameter with a special care when GLR is used, since actions will be called even on trees that can't be completed (unsuccessful parses).","title":"call_actions_during_tree_build"},{"location":"parser/#consume_input","text":"A boolean whose value is True by default. If True the whole input must be consumed for the parse to be considered successful. This is most of the time what you want. If set to False then LR parser will parse as much as possible and leave the rest of the input unconsumed while GLR parser will produce all possible parses with both completely and incompletely consumed input. Warning Be aware that setting this option to False for GLR usually leads to high level of ambiguity and multiple parses as any substring from beginning of the input that parses will be considered a valid parse.","title":"consume_input"},{"location":"parser/#prefer_shifts","text":"By default set to True for LR parser and to False for GLR parser. In case of shift/reduce conflicts this strategy would favor shift over reduce. You can still use associativity rules to decide per production. You can disable this rule on per-production basis by using nops on the production. Warning Do not use prefer_shifts if you don't understand the implications. Try to understand conflicts and resolution strategies .","title":"prefer_shifts"},{"location":"parser/#prefer_shifts_over_empty","text":"By default set to True for LR parser and to False for GLR parser. In case of shift/reduce conflicts on empty reductions this strategy would favor shift over reduce. You can still use associativity rules to decide per production. You can disable this rule on per-production basis by using nopse on the production. Warning Do not use prefer_shifts_over_empty if you don't understand the implications. Try to understand conflicts and resolution strategies .","title":"prefer_shifts_over_empty"},{"location":"parser/#error_recovery","text":"By default set to False . If set to True default error recovery will be used. If set to a Python function, the function will be called to recover from errors. For more information see Error recovery .","title":"error_recovery"},{"location":"parser/#debugdebug_layout","text":"This parameter if set to True will put the parser in debug mode. In this mode parser will print a detailed information of its actions to the standard output. To put layout subparser in the debug mode use the debug_layout parameter. Both parameters are set to False by default. For more information see Debugging","title":"debug/debug_layout"},{"location":"parser/#debug_colors","text":"Set this to True to enable terminal colors in debug/trace output. False by default.","title":"debug_colors"},{"location":"parser/#tables","text":"The value of this parameter is either parglare.LALR or parglare.SLR and it is used to choose the type of LR tables to create. By default LALR tables are used with a slight twist to avoid Reduce/Reduce conflicts that may happen with pure LALR tables. This parameter should not be used in normal circumstances and is provided more for experimentation purposes.","title":"tables"},{"location":"parser/#force_load_table","text":"LR table is loaded from <grammar_file_name>.pgt file if the file exists and is newer than all of the grammar files, root and imported. If any of the grammar file modification time is greater than the modification time of the cached LR table file, table is recalculated and persisted. If you are deploying the parser in a way that will change file modification times which would trigger table calculation you can set force_load_table to True . If this flag is set no modification check will be performed and table calculation will happen only if .pgt file doesn't exist.","title":"force_load_table"},{"location":"parser/#table","text":"You can pass precomputed parsing table here. This is useful for implementing custom parse table caching. None value for this parameter (the default) instructs parser to build (or fetch from cache) it's own tables internally. Example flow for custom caching is shown in an example . Warning Be careful to provide parse tables compatible with parser type. Passing tables containing conflicts to Parser class will probably result in an error, but passing tables with automatically resolved conflicts ( prefer_shifts=True ) to GLRParser will result in parser which may skip proper parses.","title":"table"},{"location":"parser/#parse-and-parse_file-calls","text":"parse call is used to parse input string or list of objects. For parsing of textual file parse_file is used. These two calls accepts the following parameters: input_str - first positional and mandatory parameter only for parse call - the input string/list of objects. position - the start position to parse from. By default 0. extra - an object used for arbitrary user state kept during parsing. It will be accessible on context-like objects. If not given an instance of dict will be created. file_name - first positional and mandatory parameter only for parse_file call - the name/path of the file to parse.","title":"parse and parse_file calls"},{"location":"parser/#token-class","text":"This class from parglare.parser is used to represent lookahead tokens. Token is a concrete matched terminal from the input stream. Attributes: symbol ( Terminal ) - terminal grammar symbol represented by this token, value ( list or str ) - matched part of the input stream, additional_data ( list ) - additional information returned by a custom recognizer. length ( int ) - length of the matched input.","title":"Token class"},{"location":"pglr/","text":"The pglr command \u00b6 pglr CLI command is available when parglare is installed. This command is used to debug the grammar, visualize the LR automata and make a visual trace of the GLR parsing. To get the help on the command run: $ pglr Usage: pglr [OPTIONS] COMMAND [ARGS]... Command line interface for working with parglare grammars. Options: --debug Debug/trace output. --no-colors Disable output coloring. --prefer-shifts Prefer shifts over reductions. --prefer-shifts-over-empty Prefer shifts over empty reductions. --help Show this message and exit. Commands: compile parse trace viz Compiling the grammar \u00b6 compile command is used for checking the grammar, reporting conflicts and producing LR table .pgt files. It is not mandatory to compile the grammar as parglare will calculate table during parser construction if .pgt file doesn't exist or is not newer than all of the grammar files. But it is recommended to use this command during development to investigate possible conflicts and calculate table in advance. To get help on the command run: $ pglr compile --help Usage: pglr compile [OPTIONS] GRAMMAR_FILE Options: --help Show this message and exit. To compile and check your grammar run: $ pglr compile <grammar_file> where <grammar_file> is the path to your grammar file. If there is no error in the grammar you will get Grammar OK. message. In case of LR conflicts you will get a detailed information on all Shift/Reduce and Reduce/Reduce conflicts which makes much easier to see the exact cause of ambiguity and to use disambiguation rules to resolve the conflicts or to go with GLR if the grammar is not LR(1). In case of error you will get error message with the information what is the error and where it is in the grammar. For example: $ pglr compile calc.pg Error in the grammar file. Error in file \"calc.pg\" at position 4,16 => \"/' E left*, 2}\\n | E \". Expected: { or | or ; or Name or RegExTerm or StrTerm Tip Be sure to deploy .pgt file to production as you will avoid unnecessary table calculation on the first run. Furthermore, if parglare can't write to .pgt file due to permission it will resort to calculating LR table whenever started. Getting detailed information about the grammar \u00b6 If there is a conflict in the LR table, by default, you get the information about the state in which conflict occurs, lookaheads and actions. To get the full information about the grammar you can run pglr command in the debug mode. $ pglr --debug compile calc.pg *** GRAMMAR *** Terminals: number STOP + - ^ EMPTY ) \\d+(\\.\\d+)? ( / * NonTerminals: S' E Productions: 0: S' = E STOP 1: E = E + E 2: E = E - E 3: E = E * E 4: E = E / E 5: E = E ^ E 6: E = ( E ) 7: E = number *** STATES *** State 0 0: S' = . E STOP {} 1: E = . E + E {STOP, -, +, ^, ), /, *} 2: E = . E - E {STOP, -, +, ^, ), /, *} 3: E = . E * E {STOP, -, +, ^, ), /, *} 4: E = . E / E {STOP, -, +, ^, ), /, *} 5: E = . E ^ E {STOP, -, +, ^, ), /, *} 6: E = . ( E ) {STOP, -, +, ^, ), /, *} 7: E = . number {STOP, -, +, ^, ), /, *} GOTO: E->1 ACTIONS: (->SHIFT:2, number->SHIFT:3 ... This will give enumerated all the productions of your grammars and all the states. For each state you get the LR items with lookahead, elements of GOTO table and elements of ACTIONS table. In the previous example state 0 will have a transition to state 1 when E is reduced, transition to state 2 if ( can be shifted and transition to state 3 if number can be shifted. Tip You can use --debug option with any pglr command to put the parser in the debug mode and get a detailed output. Visualizing LR automata \u00b6 To visualize your automata with all the states and possible transitions run the command: $ pglr viz calc.pg Grammar OK. Generating 'calc.pg.dot' file for the grammar PDA. Use dot viewer (e.g. xdot) or convert to pdf by running 'dot -Tpdf -O calc.pg.dot' As given in the output you will get a dot file which represents LR automaton visualization. You can see this diagram using dot viewers (e.g. xdot ) or you can transform it to other file formats using the dot tool (you'll have to install Graphviz software for that). This is an example of LR automaton visualization for the calc grammar from the quick intro (click on the image to enlarge): Tracing GLR parsing \u00b6 GLR parser uses a graph-like stack ( Graph-Structured Stack - GSS ) and to understand what's going on during GLR operation GLR parser and pglr command provide a way to trace the GSS. To get a help on the trace command run: $ pglr trace --help Usage: pglr trace [OPTIONS] GRAMMAR_FILE Options: -f, --input-file PATH Input file for tracing -i, --input TEXT Input string for tracing -r, --frontiers Align GSS nodes into frontiers (token levels) --help Show this message and exit. You either give your input using file ( -f ) or using string provided in the command ( -i ), but not both. To run the GLR trace for the calc grammar and some input: $ pglr trace calc.pg --frontiers -i \"2 + 3 * 5\" The -i switch tells the command to treat the last parameter as the input string to parse. Tip Since the GSS can be quite large and complex for larger inputs the advice is to use a minimal input that will exhibit the intended behavior for a visualization to be usable. The trace sub-command implies --debug switch so the parser will run in the debug mode and will produce the detailed output on the grammar, LR automata and the parsing process. Additionally, a dot file will be created, with the name parglare_trace.dot if input is given on command line or <input_file_name>_trace.dot if input is given as a file. The dot file can be visualized using dot viewers or transformed to other file formats using the dot tool. If --frontiers flag is given the GSS nodes will be organized in frontiers from left to right. For the command above and grammar (file calc.pg ) E: E \"+\" E | E \"*\" E | number; terminals number: /\\d+/; GLR trace visualization will be (click on the image to enlarge): Dotted red arrows represent each step in the parsing process. They are numbered consecutively. After the ordinal number is the action (either S -Shift or R -reduce). For shift action a grammar symbol and the shifted value is given. For reduction a production is given and the resulting head will have a parent node closer to the beginning. Black solid arrows are the links to the parent node in the GSS. The number at these links represents the ambiguity i.e. the number of different derivation. We can see that in this example we have two solutions as we haven't provided the priority of operations. Tip To produce GLR parser visual trace from code your must put the parser in debug mode by setting debug to True and enable visual tracing by setting debug_trace to True . Parsing inputs \u00b6 You can use pglr command to parse input strings and file and produce parse forests/trees. For example: $ pglr parse calc.pg -i \"1 + 2 * 3\" --glr --dot Solutions:2 Ambiguities:1 Printing the forest: E - ambiguity[2] 1:E[0->9] E[0->5] E[0->1] number[0->1, \"1\"] +[2->3, \"+\"] E[4->5] number[4->5, \"2\"] *[6->7, \"*\"] E[8->9] number[8->9, \"3\"] 2:E[0->9] E[0->1] number[0->1, \"1\"] +[2->3, \"+\"] E[4->9] E[4->5] number[4->5, \"2\"] *[6->7, \"*\"] E[8->9] number[8->9, \"3\"] Created dot file forest.dot We provided --glr switch to put he parser in GLR mode and --dot switch to produce graphical visualization of the parse forest. We see that this input has 2 solutions and 1 ambiguity. The ambiguity is indicated in the parse tree as E - ambiguity[2] telling us that the E symbol is ambiguous at this level with 2 different interpretations ( 1: and 2: bellow). We can see also that forest.dot is provided which looks like this:","title":"pglr command"},{"location":"pglr/#the-pglr-command","text":"pglr CLI command is available when parglare is installed. This command is used to debug the grammar, visualize the LR automata and make a visual trace of the GLR parsing. To get the help on the command run: $ pglr Usage: pglr [OPTIONS] COMMAND [ARGS]... Command line interface for working with parglare grammars. Options: --debug Debug/trace output. --no-colors Disable output coloring. --prefer-shifts Prefer shifts over reductions. --prefer-shifts-over-empty Prefer shifts over empty reductions. --help Show this message and exit. Commands: compile parse trace viz","title":"The pglr command"},{"location":"pglr/#compiling-the-grammar","text":"compile command is used for checking the grammar, reporting conflicts and producing LR table .pgt files. It is not mandatory to compile the grammar as parglare will calculate table during parser construction if .pgt file doesn't exist or is not newer than all of the grammar files. But it is recommended to use this command during development to investigate possible conflicts and calculate table in advance. To get help on the command run: $ pglr compile --help Usage: pglr compile [OPTIONS] GRAMMAR_FILE Options: --help Show this message and exit. To compile and check your grammar run: $ pglr compile <grammar_file> where <grammar_file> is the path to your grammar file. If there is no error in the grammar you will get Grammar OK. message. In case of LR conflicts you will get a detailed information on all Shift/Reduce and Reduce/Reduce conflicts which makes much easier to see the exact cause of ambiguity and to use disambiguation rules to resolve the conflicts or to go with GLR if the grammar is not LR(1). In case of error you will get error message with the information what is the error and where it is in the grammar. For example: $ pglr compile calc.pg Error in the grammar file. Error in file \"calc.pg\" at position 4,16 => \"/' E left*, 2}\\n | E \". Expected: { or | or ; or Name or RegExTerm or StrTerm Tip Be sure to deploy .pgt file to production as you will avoid unnecessary table calculation on the first run. Furthermore, if parglare can't write to .pgt file due to permission it will resort to calculating LR table whenever started.","title":"Compiling the grammar"},{"location":"pglr/#getting-detailed-information-about-the-grammar","text":"If there is a conflict in the LR table, by default, you get the information about the state in which conflict occurs, lookaheads and actions. To get the full information about the grammar you can run pglr command in the debug mode. $ pglr --debug compile calc.pg *** GRAMMAR *** Terminals: number STOP + - ^ EMPTY ) \\d+(\\.\\d+)? ( / * NonTerminals: S' E Productions: 0: S' = E STOP 1: E = E + E 2: E = E - E 3: E = E * E 4: E = E / E 5: E = E ^ E 6: E = ( E ) 7: E = number *** STATES *** State 0 0: S' = . E STOP {} 1: E = . E + E {STOP, -, +, ^, ), /, *} 2: E = . E - E {STOP, -, +, ^, ), /, *} 3: E = . E * E {STOP, -, +, ^, ), /, *} 4: E = . E / E {STOP, -, +, ^, ), /, *} 5: E = . E ^ E {STOP, -, +, ^, ), /, *} 6: E = . ( E ) {STOP, -, +, ^, ), /, *} 7: E = . number {STOP, -, +, ^, ), /, *} GOTO: E->1 ACTIONS: (->SHIFT:2, number->SHIFT:3 ... This will give enumerated all the productions of your grammars and all the states. For each state you get the LR items with lookahead, elements of GOTO table and elements of ACTIONS table. In the previous example state 0 will have a transition to state 1 when E is reduced, transition to state 2 if ( can be shifted and transition to state 3 if number can be shifted. Tip You can use --debug option with any pglr command to put the parser in the debug mode and get a detailed output.","title":"Getting detailed information about the grammar"},{"location":"pglr/#visualizing-lr-automata","text":"To visualize your automata with all the states and possible transitions run the command: $ pglr viz calc.pg Grammar OK. Generating 'calc.pg.dot' file for the grammar PDA. Use dot viewer (e.g. xdot) or convert to pdf by running 'dot -Tpdf -O calc.pg.dot' As given in the output you will get a dot file which represents LR automaton visualization. You can see this diagram using dot viewers (e.g. xdot ) or you can transform it to other file formats using the dot tool (you'll have to install Graphviz software for that). This is an example of LR automaton visualization for the calc grammar from the quick intro (click on the image to enlarge):","title":"Visualizing LR automata"},{"location":"pglr/#tracing-glr-parsing","text":"GLR parser uses a graph-like stack ( Graph-Structured Stack - GSS ) and to understand what's going on during GLR operation GLR parser and pglr command provide a way to trace the GSS. To get a help on the trace command run: $ pglr trace --help Usage: pglr trace [OPTIONS] GRAMMAR_FILE Options: -f, --input-file PATH Input file for tracing -i, --input TEXT Input string for tracing -r, --frontiers Align GSS nodes into frontiers (token levels) --help Show this message and exit. You either give your input using file ( -f ) or using string provided in the command ( -i ), but not both. To run the GLR trace for the calc grammar and some input: $ pglr trace calc.pg --frontiers -i \"2 + 3 * 5\" The -i switch tells the command to treat the last parameter as the input string to parse. Tip Since the GSS can be quite large and complex for larger inputs the advice is to use a minimal input that will exhibit the intended behavior for a visualization to be usable. The trace sub-command implies --debug switch so the parser will run in the debug mode and will produce the detailed output on the grammar, LR automata and the parsing process. Additionally, a dot file will be created, with the name parglare_trace.dot if input is given on command line or <input_file_name>_trace.dot if input is given as a file. The dot file can be visualized using dot viewers or transformed to other file formats using the dot tool. If --frontiers flag is given the GSS nodes will be organized in frontiers from left to right. For the command above and grammar (file calc.pg ) E: E \"+\" E | E \"*\" E | number; terminals number: /\\d+/; GLR trace visualization will be (click on the image to enlarge): Dotted red arrows represent each step in the parsing process. They are numbered consecutively. After the ordinal number is the action (either S -Shift or R -reduce). For shift action a grammar symbol and the shifted value is given. For reduction a production is given and the resulting head will have a parent node closer to the beginning. Black solid arrows are the links to the parent node in the GSS. The number at these links represents the ambiguity i.e. the number of different derivation. We can see that in this example we have two solutions as we haven't provided the priority of operations. Tip To produce GLR parser visual trace from code your must put the parser in debug mode by setting debug to True and enable visual tracing by setting debug_trace to True .","title":"Tracing GLR parsing"},{"location":"pglr/#parsing-inputs","text":"You can use pglr command to parse input strings and file and produce parse forests/trees. For example: $ pglr parse calc.pg -i \"1 + 2 * 3\" --glr --dot Solutions:2 Ambiguities:1 Printing the forest: E - ambiguity[2] 1:E[0->9] E[0->5] E[0->1] number[0->1, \"1\"] +[2->3, \"+\"] E[4->5] number[4->5, \"2\"] *[6->7, \"*\"] E[8->9] number[8->9, \"3\"] 2:E[0->9] E[0->1] number[0->1, \"1\"] +[2->3, \"+\"] E[4->9] E[4->5] number[4->5, \"2\"] *[6->7, \"*\"] E[8->9] number[8->9, \"3\"] Created dot file forest.dot We provided --glr switch to put he parser in GLR mode and --dot switch to produce graphical visualization of the parse forest. We see that this input has 2 solutions and 1 ambiguity. The ambiguity is indicated in the parse tree as E - ambiguity[2] telling us that the E symbol is ambiguous at this level with 2 different interpretations ( 1: and 2: bellow). We can see also that forest.dot is provided which looks like this:","title":"Parsing inputs"},{"location":"recognizers/","text":"Recognizers \u00b6 Parglare uses scannerless parsing. Actually, scanner is integrated in the parser. Each token is created/recognized in the input during parsing using so called recognizer which is connected to the grammar terminal symbol. This gives us greater flexibility. First, recognizing tokens during parsing eliminate lexical ambiguities that arise in separate scanning due to the lack of parsing context. Second, having a separate recognizers for grammar terminal symbols allows us to parse not only text but a stream of anything as parsing is nothing more but constructing a tree (or some other form) out of a flat list of objects. Those objects are characters if text is parsed, but don't have to be. Parglare has two built-in recognizers for textual parsing that can be specified in the grammar directly . Those are usually enough if text is parsed, but if a non-textual content is parsed you will have to supply your own recognizers that are able to recognize tokens in the input stream of objects. Recognizers are Python callables of the following form: def some_recognizer(context, input, pos): ... ... return part of input starting at pos where context is the parsing context object and is optional (e.g. you don't have to accept it in your recognizers), input is the input string or list of objects and position is the position in the input where match should be performed. The recognizer should return the part of the input that is recognized or None if it doesn't recognize anything at the current position. For example, if we have an input stream of objects that are comparable (e.g. numbers) and we want to recognize ascending elements starting at the given position but such that the recognized token must have at least two object from the input, we could write the following: def ascending_nosingle(input, pos): \"Match sublist of ascending elements. Matches at least two.\" last = pos + 1 while last < len(input) and input[last] > input[last-1]: last += 1 if last - pos >= 2: return input[pos:last] We register our recognizers during grammar construction. All terminal rules in the grammar that don't define string or regex match (i.e. they have empty bodies) must be augmented with custom recognizers for the parser to be complete. In order to do that, create a Python dict where the key will be a rule name used in the grammar references and the value will be recognizer callable. Pass the dictionary as a recognizers parameter to the parser. recognizers = { 'ascending': ascending_nosingle } grammar = Grammar.from_file('mygrammar.pg', recognizers=recognizers) In the file mygrammar.pg you have to provide a terminal rule with empty body: ascending: ; Tip You can also define recognizers in a separate Python file that accompanies your grammar file. In that case, recognizers will be automatically registered on the parser. For more information see grammar file recognizers . There is a need sometimes to pass additional data from recognizers to appropriate actions . If you need this you can return additional information after the matched part of the input. For example: def a_rec(input, pos): m = re.compile(r'(\\d+)') result = m.match(input[pos:]) return result.group(), result This recognizer returns both the string it matched and the resulting regex match so that action can use the match object to extract more information without repeating the match: def a_act(context, value, match): \"\"\" Action will get the additional returned information from the a_rec recognizer. In this case a regex match object. \"\"\" # Do something with `value` and `match` and create the result You can return more than one additional element. Essentially if a tuple is returned by the recognizer, the first element has to be matched input while the rest is additional data. If you are building parse tree , additional information returned by recognizers is kept on NodeTerm (and Token ) as additional_data attribute which is a list of all additional info returned by the recognizer. Tip If you want more information you can investigate recognizer tests .","title":"Recognizers"},{"location":"recognizers/#recognizers","text":"Parglare uses scannerless parsing. Actually, scanner is integrated in the parser. Each token is created/recognized in the input during parsing using so called recognizer which is connected to the grammar terminal symbol. This gives us greater flexibility. First, recognizing tokens during parsing eliminate lexical ambiguities that arise in separate scanning due to the lack of parsing context. Second, having a separate recognizers for grammar terminal symbols allows us to parse not only text but a stream of anything as parsing is nothing more but constructing a tree (or some other form) out of a flat list of objects. Those objects are characters if text is parsed, but don't have to be. Parglare has two built-in recognizers for textual parsing that can be specified in the grammar directly . Those are usually enough if text is parsed, but if a non-textual content is parsed you will have to supply your own recognizers that are able to recognize tokens in the input stream of objects. Recognizers are Python callables of the following form: def some_recognizer(context, input, pos): ... ... return part of input starting at pos where context is the parsing context object and is optional (e.g. you don't have to accept it in your recognizers), input is the input string or list of objects and position is the position in the input where match should be performed. The recognizer should return the part of the input that is recognized or None if it doesn't recognize anything at the current position. For example, if we have an input stream of objects that are comparable (e.g. numbers) and we want to recognize ascending elements starting at the given position but such that the recognized token must have at least two object from the input, we could write the following: def ascending_nosingle(input, pos): \"Match sublist of ascending elements. Matches at least two.\" last = pos + 1 while last < len(input) and input[last] > input[last-1]: last += 1 if last - pos >= 2: return input[pos:last] We register our recognizers during grammar construction. All terminal rules in the grammar that don't define string or regex match (i.e. they have empty bodies) must be augmented with custom recognizers for the parser to be complete. In order to do that, create a Python dict where the key will be a rule name used in the grammar references and the value will be recognizer callable. Pass the dictionary as a recognizers parameter to the parser. recognizers = { 'ascending': ascending_nosingle } grammar = Grammar.from_file('mygrammar.pg', recognizers=recognizers) In the file mygrammar.pg you have to provide a terminal rule with empty body: ascending: ; Tip You can also define recognizers in a separate Python file that accompanies your grammar file. In that case, recognizers will be automatically registered on the parser. For more information see grammar file recognizers . There is a need sometimes to pass additional data from recognizers to appropriate actions . If you need this you can return additional information after the matched part of the input. For example: def a_rec(input, pos): m = re.compile(r'(\\d+)') result = m.match(input[pos:]) return result.group(), result This recognizer returns both the string it matched and the resulting regex match so that action can use the match object to extract more information without repeating the match: def a_act(context, value, match): \"\"\" Action will get the additional returned information from the a_rec recognizer. In this case a regex match object. \"\"\" # Do something with `value` and `match` and create the result You can return more than one additional element. Essentially if a tuple is returned by the recognizer, the first element has to be matched input while the rest is additional data. If you are building parse tree , additional information returned by recognizers is kept on NodeTerm (and Token ) as additional_data attribute which is a list of all additional info returned by the recognizer. Tip If you want more information you can investigate recognizer tests .","title":"Recognizers"},{"location":"about/CONTRIBUTING/","text":"Contributing \u00b6 Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways: Types of Contributions \u00b6 Report Bugs \u00b6 Report bugs at https://github.com/igordejanovic/parglare/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug. Fix Bugs \u00b6 Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it. Implement Features \u00b6 Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it. Write Documentation \u00b6 parglare could always use more documentation, whether as part of the official parglare docs, in docstrings, or even on the web in blog posts, articles, and such. Submit Feedback \u00b6 The best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :) Get Started! \u00b6 Ready to contribute? Here's how to set up parglare for local development. Fork the parglare repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/parglare.git Install your local copy into a virtual environment. This is how you set up your fork for local development: $ cd parglare/ $ python -m venv venv $ source venv/bin/activate $ ./install-dev.sh This is needed just the first time. To work on parglare later you just need to activate the virtual environment for each new terminal session: $ cd parglare/ $ source venv/bin/activate Create a branch for local development:: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, run tests: $ ./runtests.sh and verify that all tests pass. Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Check this on how to write nice git log messages. Submit a pull request through the GitHub website. CI will run the tests for all supported Python versions. Check in the GitHub UI that all pipelines pass. Pull Request Guidelines \u00b6 Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds/changes functionality, the docs should be updated. The pull request should work for Python 3.8-3.12. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and make sure that the tests pass for all supported Python versions. Tips \u00b6 To run a subset of tests: $ py.test tests/func/mytest.py or a single test: $ py.test tests/func/mytest.py::some_test","title":"Contributing"},{"location":"about/CONTRIBUTING/#contributing","text":"Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given. You can contribute in many ways:","title":"Contributing"},{"location":"about/CONTRIBUTING/#types-of-contributions","text":"","title":"Types of Contributions"},{"location":"about/CONTRIBUTING/#report-bugs","text":"Report bugs at https://github.com/igordejanovic/parglare/issues. If you are reporting a bug, please include: Your operating system name and version. Any details about your local setup that might be helpful in troubleshooting. Detailed steps to reproduce the bug.","title":"Report Bugs"},{"location":"about/CONTRIBUTING/#fix-bugs","text":"Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.","title":"Fix Bugs"},{"location":"about/CONTRIBUTING/#implement-features","text":"Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.","title":"Implement Features"},{"location":"about/CONTRIBUTING/#write-documentation","text":"parglare could always use more documentation, whether as part of the official parglare docs, in docstrings, or even on the web in blog posts, articles, and such.","title":"Write Documentation"},{"location":"about/CONTRIBUTING/#submit-feedback","text":"The best way to send feedback is to file an issue at https://github.com/igordejanovic/parglare/issues. If you are proposing a feature: Explain in detail how it would work. Keep the scope as narrow as possible, to make it easier to implement. Remember that this is a volunteer-driven project, and that contributions are welcome :)","title":"Submit Feedback"},{"location":"about/CONTRIBUTING/#get-started","text":"Ready to contribute? Here's how to set up parglare for local development. Fork the parglare repo on GitHub. Clone your fork locally: $ git clone git@github.com:your_name_here/parglare.git Install your local copy into a virtual environment. This is how you set up your fork for local development: $ cd parglare/ $ python -m venv venv $ source venv/bin/activate $ ./install-dev.sh This is needed just the first time. To work on parglare later you just need to activate the virtual environment for each new terminal session: $ cd parglare/ $ source venv/bin/activate Create a branch for local development:: $ git checkout -b name-of-your-bugfix-or-feature Now you can make your changes locally. When you're done making changes, run tests: $ ./runtests.sh and verify that all tests pass. Commit your changes and push your branch to GitHub: $ git add . $ git commit -m \"Your detailed description of your changes.\" $ git push origin name-of-your-bugfix-or-feature Check this on how to write nice git log messages. Submit a pull request through the GitHub website. CI will run the tests for all supported Python versions. Check in the GitHub UI that all pipelines pass.","title":"Get Started!"},{"location":"about/CONTRIBUTING/#pull-request-guidelines","text":"Before you submit a pull request, check that it meets these guidelines: The pull request should include tests. If the pull request adds/changes functionality, the docs should be updated. The pull request should work for Python 3.8-3.12. Check https://travis-ci.org/igordejanovic/parglare/pull_requests and make sure that the tests pass for all supported Python versions.","title":"Pull Request Guidelines"},{"location":"about/CONTRIBUTING/#tips","text":"To run a subset of tests: $ py.test tests/func/mytest.py or a single test: $ py.test tests/func/mytest.py::some_test","title":"Tips"},{"location":"about/LICENSE/","text":"MIT License Copyright (c) 2016-2018, Igor R. Dejanovi\u0107 and contributors Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"release_notes/release_0_14/","text":"Release notes for 0.14 \u00b6 Parenthesized groups \u00b6 RHS in grammar rules now can used parentheses to group element. These groups behave similar to any other rule reference. E.g. repetitions and assignments can be applied. Previously: S: a_then_b*[comma] c; a_then_b: a b; ... Now you can write: S: (a b)*[comma] c; ... You can nest groups, combine with choice operator etc. S: ( (a c)+ | b)*[comma] c; For more info see a new section in the docs . GLR forest \u00b6 GLR now returns Forest object. This object represents all the possible solutions. Forest can be iterated, indexed, yielding lazy parse trees. See more info in the docs . Extensions to pglr command \u00b6 pglr trace now provides --frontier flag to organize GSS nodes into frontiers. See the docs . pglr parse is added for parsing input strings and files and producing forests and trees as either string or graphical dot representation. See the docs . Support for visitor \u00b6 Visitor pattern is supported as a visitor function enabling depth-first processing of tree-like structures. See the docs . New examples \u00b6 Several new examples are added: JSON BibTeX Java (based on Java SE 16 version) Performance tests \u00b6 New performance tests based on new example grammars are provided in tests/perf . Run runall.sh and read the reports in tests/perf/reports .","title":0.14},{"location":"release_notes/release_0_14/#release-notes-for-014","text":"","title":"Release notes for 0.14"},{"location":"release_notes/release_0_14/#parenthesized-groups","text":"RHS in grammar rules now can used parentheses to group element. These groups behave similar to any other rule reference. E.g. repetitions and assignments can be applied. Previously: S: a_then_b*[comma] c; a_then_b: a b; ... Now you can write: S: (a b)*[comma] c; ... You can nest groups, combine with choice operator etc. S: ( (a c)+ | b)*[comma] c; For more info see a new section in the docs .","title":"Parenthesized groups"},{"location":"release_notes/release_0_14/#glr-forest","text":"GLR now returns Forest object. This object represents all the possible solutions. Forest can be iterated, indexed, yielding lazy parse trees. See more info in the docs .","title":"GLR forest"},{"location":"release_notes/release_0_14/#extensions-to-pglr-command","text":"pglr trace now provides --frontier flag to organize GSS nodes into frontiers. See the docs . pglr parse is added for parsing input strings and files and producing forests and trees as either string or graphical dot representation. See the docs .","title":"Extensions to pglr command"},{"location":"release_notes/release_0_14/#support-for-visitor","text":"Visitor pattern is supported as a visitor function enabling depth-first processing of tree-like structures. See the docs .","title":"Support for visitor"},{"location":"release_notes/release_0_14/#new-examples","text":"Several new examples are added: JSON BibTeX Java (based on Java SE 16 version)","title":"New examples"},{"location":"release_notes/release_0_14/#performance-tests","text":"New performance tests based on new example grammars are provided in tests/perf . Run runall.sh and read the reports in tests/perf/reports .","title":"Performance tests"},{"location":"release_notes/release_0_15/","text":"Release notes for 0.15 \u00b6 This release should be fully backward compatible so the upgrade should require no changes. Greedy repetitions \u00b6 The most important new feature in this release is a support for greedy repetition. Read more in the docs . New way to disambiguate the GLR forest \u00b6 A new and recommended way for dynamic disambiguation is by using forest.disambiguate . Read more in the docs . Optimized getting of the first tree from the GLR forest \u00b6 If you are not interested into analyzing the forest and comparing trees but just want to get any valid tree you can use forest.get_first_tree() which is optimized to avoid tree enumeration that might be costly. The returned tree is fully unpacked and doesn't use proxies, i.e. it contains only NodeTerm and NodeNonTerm instances.","title":0.15},{"location":"release_notes/release_0_15/#release-notes-for-015","text":"This release should be fully backward compatible so the upgrade should require no changes.","title":"Release notes for 0.15"},{"location":"release_notes/release_0_15/#greedy-repetitions","text":"The most important new feature in this release is a support for greedy repetition. Read more in the docs .","title":"Greedy repetitions"},{"location":"release_notes/release_0_15/#new-way-to-disambiguate-the-glr-forest","text":"A new and recommended way for dynamic disambiguation is by using forest.disambiguate . Read more in the docs .","title":"New way to disambiguate the GLR forest"},{"location":"release_notes/release_0_15/#optimized-getting-of-the-first-tree-from-the-glr-forest","text":"If you are not interested into analyzing the forest and comparing trees but just want to get any valid tree you can use forest.get_first_tree() which is optimized to avoid tree enumeration that might be costly. The returned tree is fully unpacked and doesn't use proxies, i.e. it contains only NodeTerm and NodeNonTerm instances.","title":"Optimized getting of the first tree from the GLR forest"}]}